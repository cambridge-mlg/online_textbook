{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Classification is a class of supervised machine learning tasks in which the aim is to predict a **discrete valued output** \\\\(y^\\star\\\\) at an **input** location \\\\(x^\\star\\\\) using a **training set of input-output pairs** \\\\(\\{x_n, y_n\\}_{n=1}^N\\\\) pairs. Classification is therefore similar to __[regression](regression_intro.ipynb)__, but regression considers continuous valued outputs rather than discrete ones. \n",
    "\n",
    "Here's a cartoon classification task where the outputs take the value $0$ or $1$. Often the classification task involves returning to the user a probability distribution $p(y^\\star|x^\\star)$, here shown in blue, which indicates the probability of observing each possible output value at an input $x^\\star$.\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img src=\"intro-classification.svg\" alt=\"Snow\" style=\"width:80%; float: center; padding: 0px; padding : 20px\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Like regression, classification encompasses many different types of input e.g. they can be scalar or multi-dimensional; real valued or discrete. \n",
    "\n",
    "Example classification problems include:\n",
    "\n",
    "\n",
    "|  Application | Inputs | Outputs | \n",
    "| :---: | :---: | :---: | :---: |\n",
    "| email spam filtering | email message | spam / not spam |\n",
    "| medical diagnosis | patient medical data | disease diagnosis |\n",
    "| object recognition | image RGB pixel values | object present in scene |  \n",
    "| speech recognition | audio waveform | words spoken | \n",
    "| machine translation | text in language 1 | text in language 2 |\n",
    "\n",
    "There are many different subclasses of classification problem. **Binary classification** problems  just have two possible output values (e.g. spam vs non-spam), but **multi-class classification** problems have many possible output values (e.g. object recognition with a single object in each image). **Multi-label classification**, analogous to multi-ouput regression, has multi-dimensional outputs (i.e. multiple labels per input, e.g. object recognition with many objects in each image). Multi-label classification tasks in which there strong dependencies between the output dimensions belong to the class of **structured prediction** problems (e.g. machine translation whether the output is a sequence of words).\n",
    "\n",
    "The focus of many classification tasks is on predicting the output, but for many the goal (or subgoal) is to understand the relationship between the inputs and the outputs. For example, classification can be used to determine what inputs effect whether someone votes Democrat or Republican.  \n",
    "\n",
    "Our tour of classification will cover many of the same cross-cutting concepts we met in regression, including *generative models*, *maximum likelihood estimation*, *overfitting* and *probabilistic inference*. Whereas many regression models lead to analytic parameter estimates and predictions, classification models typically do not and so *optimisation methods* will be needed for maximum likelihood estimation and *approximation methods* for probabilistic inference.\n",
    "\n",
    "\n",
    "## Outline of this section\n",
    "\n",
    "  \n",
    "1. Binary logisting classification\n",
    "\n",
    "    a. [Understanding the binary logisitic classification model](classification_logistic_regression_model.ipynb) \n",
    "    \n",
    "    b. [Fitting the binary logisitic classification model using maximum likelihood](classification_logistic_regression_ML_fitting.ipynb) \n",
    "    \n",
    "    c. [A case study: applying binary classification to the iris dataset](classification_gradient_case_study.ipynb)\n",
    "    <br><br>\n",
    "\n",
    "2. [Multi-class logistic classification](classification_multiclass.ipynb) \n",
    "\n",
    "  Logistic regression for multiple classes <br><br>\n",
    "\n",
    "3. [Non-linear classification](classification_non-linear.ipynb)\n",
    "\n",
    "  Handling non-linear decision boundaries. <br><br>    \n",
    "  \n",
    "4. [Bayesian logistic regression](classification_bayesian.ipynb)\n",
    "\n",
    "  Baysian approaches for capturing uncertainty in the parameters. <br><br>   \n",
    "\n",
    "5. [k-nearest neighbours](classification_knn.ipynb) \n",
    "\n",
    "  A simple approach to classification that uses the class labels of nearby training datapoints <br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
