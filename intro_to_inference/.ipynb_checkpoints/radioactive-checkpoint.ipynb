{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-b8f3eebaeaf6>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-b8f3eebaeaf6>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    from ../helper_functions import * # import some helper functions used for convenience - you don't need to worry about this\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'svg' # change output plot format to 'svg' - you don't need to worry about this\n",
    "# In subsequent demos the next import statements will be hidden either\n",
    "# within the helper_functions import or by hiding the code cell\n",
    "from ../helper_functions import * # import some helper functions used for convenience - you don't need to worry about this\n",
    "import helper_functions as h\n",
    "\n",
    "set_notebook_preferences() # set things like fonts etc - comes from helper_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to inference: A radioactive decay inference problem\n",
    "\n",
    "In this section we will study a simple inference problem where we have to estimate the radioactive decay constant from decay events [(MacKay, 2003)](http://www.inference.org.uk/itprnn/book.pdf). This example will introduce several of the foundational ideas behind probabilistic inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Estimating a radioactivity decay constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hide_input": true
   },
   "source": [
    "Unstable particles are emitted from a source and decay after travelling a distance $x$. The apparatus used to measure the decays can only detect them within the range $(x_{min}, x_{max})$ of the particle source. Physics tells us that $x$ follows an exponential distribution with a decay constant $\\lambda$:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(x|\\lambda) = \\frac{1}{Z(\\lambda)} \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg)\n",
    "\\end{align}$$\n",
    "\n",
    "where $Z(\\lambda)$ is a normalisation coefficient of \\\\( p(x|\\lambda) \\\\). After taking a number of measurements within the window \\\\((x_{min}, x_{max}) = (5, 50)\\\\) we end up with the dataset shown below\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img src=\"radio_numberline-mod.svg\" alt=\"Snow\" style=\"width:80%; float: center; padding: 0px; padding : 20px\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "**How can we infer the value of $\\boldsymbol{\\lambda}$ from these measurements**? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to begin by considering two heuristic approaches to this inference problem in order to understand what properties an \"ideal\" approach might have. When I ask people to suggest heuristic approaches they usually fall into two categories: those which involve computing a histogram of the data and those based around computing a statistic of the data such as the mean. Let's look at each in turn.\n",
    "\n",
    "video here\n",
    "\n",
    "To learn more about the pitfalls of heuristic approaches, please look at this [notebook](radioactive_heuristic.ipynb) which works through the heuristic methods in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Probabilistic Approach to Inference in the Radioactive Decay Example\n",
    "\n",
    "We now consider an inference method that has fewer _ad hoc_ choices. Probabilistic inference is based on two key ideas:\n",
    "\n",
    "1. the \"right\" answer to the inference problem is a probability distribution over all possible settings of the unknown variables that indicates the plausiblity of each setting given obeserved data.\n",
    "\n",
    "2. to compute the answer to an inference problem, apply the sum and product rules of probability to evaluate the plausibility of any setting of any unknown variable given observed data\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Sum rule:} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; p(A) &= \\int p(A,B) \\text{d}B\\\\\n",
    "%\n",
    "\\text{Product rule: } \\;\\;\\;\\;\\;\\;\\;\\; p(A,B) & = p(B) p(A|B) = p(A) p(B|A)\n",
    "\\end{align}\n",
    "\n",
    "**Bayes' rule** is a consequence of the product rule derived simply by dividing through by $p(B)$:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Bayes' rule:} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; p(A|B) & = \\frac{p(A) p(B|A)}{p(B)}\n",
    "\\end{align}\n",
    "\n",
    "Bayes' rule is especially useful when performing inference as the left hand side is the plausiblity of A (an unknown variable of interest) given B (observed data). The right hand side multiplies our prior knowledge about $A$ (expressed as a distribution $p(A)$) with the probabilistic relationship between $A$ and $B$ ($p(B|A)$ which corresponds to our modelling assumptions). The normalising constant, $p(B)$, ensures that the right hand side integrates to $1$ and is sometimes dropped:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Bayes' rule:} \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\color{magenta}{p(A|B)} & \\propto \\color{blue}{p(A)} \\color{red}{p(B|A)}\n",
    "\\end{align}\n",
    "\n",
    "The distributions in this equation have special names and can be interpretted as follows:\n",
    "\n",
    "<center>\n",
    "<span style=\"color:magenta\">**posterior distribution**</span> $\\propto$ <span style=\"color:blue\">**prior distribution**</span> $\\times$ <span style=\"color:red\">**likelihood of the parameters**</span>\n",
    " </center>  \n",
    "\n",
    "<center>\n",
    "<span style=\"color:magenta\">\n",
    "what we know after seeing the data </span>$\\propto$ <span style=\"color:blue\">what we knew before seeing the data </span>$\\times$ <span style=\"color:red\">what the data told us</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Wider Reading: Theoretical foundations of probablistic inference **\n",
    "\n",
    "would be nice to put this as a box somewhere with other content flowing around it.\n",
    "\n",
    "The probabilisitc approach has several elegant theoretical properties. [Cox (1946)](https://bayes.wustl.edu/etj/prob/book.pdf) showed that probabilistic inference was the only calculus that obeyed basic desireable axioms. Chief among them is consistency: that if the plausibility in an unknown quantity can be computed in different ways, that each should give the same answer. [Ramsey (1926)](https://plato.stanford.edu/entries/dutch-book/#BasiDutcBookArguForProb) showed that if you do not reason using probabilistic inference, there are set of bets called Dutch books which you would accept, but which are guaranteed to lose you money.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Application to the radioactive decay example **\n",
    "\n",
    "Let's unpack these two ideas in the context of the radioactive decay example. \n",
    "\n",
    "1. We argued earlier that we could not be precisely sure of the underlying setting of the decay constant $\\lambda$ given the observed data. Probabilistic inference acknowledges this fact and says that we should not return a single estimate of $\\lambda$, but rather a distribution $p(\\lambda | \\{ x_n \\}_{n=1}^N)$ that indicates the plausibility of any setting of $\\lambda$ given the observed data $\\{ x_n \\}_{n=1}^N$.\n",
    "\n",
    "2. In order to compute the distribution $p(\\lambda | \\mathcal{D})$ we can apply Bayes' rule with $A = \\lambda$ and $B = \\{x_{1} ... x_{N}\\} = \\{ x_n \\}_{n=1}^N$. \n",
    "\n",
    "\\begin{align}\n",
    "p(\\lambda | \\{ x_n \\}_{n=1}^N) = \\frac{1}{p(\\{ x_n \\}_{n=1}^N )} p(\\lambda) p(\\{ x_n \\}_{n=1}^N | \\lambda )\n",
    "\\end{align}\n",
    "\n",
    "As the decay events are independent given $\\lambda$ this simplifies:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\lambda | \\{ x_n \\}_{n=1}^N) = \\frac{ 1}{p(\\{ x_n \\}_{n=1}^N )} p(\\lambda) \\prod_{n=1}^N p(\\{ x_n \\}_{n=1}^N | \\lambda ) \\propto p(\\lambda) \\prod_{n=1}^N p( x_n | \\lambda )\n",
    "\\end{align}\n",
    "\n",
    "Where in the last step we have dropped the normalising constant $p(\\{ x_n \\}_{n=1}^N )$.\n",
    "\n",
    "In this way, Bayes' rule relates the posterior distribution over the decay constant to our model of the observed data $p(\\{ x_n \\}_{n=1}^N | \\lambda )$. Bayes' rule also requires we specify our prior knowledge about the decay constant, $p(\\lambda)$, which we will discuss below. Before we do this, notice that together the prior $p(\\lambda)$ and the probability of the observed data given the decay constant $p( x_n | \\lambda )$ specify a complete probabilistic recipe for generating datasets (by first sampling $\\lambda$ and then sampling $N$ observed data from $p(\\{ x_n \\}_{n=1}^N | \\lambda )$). This is sometimes called a **forward model** as it moves from the unknown variables to the observed variables. Inference is sometimes called **inverse modelling** as it moves from the observed variables to the unknown variables. \n",
    "\n",
    "**Sufficient Statistics**\n",
    "\n",
    "Now let's substitute in the expression for the model $p( x_n | \\lambda ) = \\frac{1}{Z(\\lambda)} \\exp\\left(- x_n/\\lambda \\right)$ into Bayes' rule\n",
    "\n",
    "\\begin{align}\n",
    "p(\\lambda | \\{ x_n \\}_{n=1}^N) \\propto p(\\lambda) \\prod_{n=1}^N \\left( \\frac{1}{Z(\\lambda)} \\exp\\left(- x_n/\\lambda \\right)\\right) = p(\\lambda)  \\frac{1}{Z(\\lambda)^N} \\exp\\left(-\\frac{1}{\\lambda} \\sum_{n = 1}^{N} x_n \\right)\n",
    "\\end{align}\n",
    "\n",
    "Notice then that we do not need to store all of the individual datapoints $\\{ x_n \\}_{n=1}^N$ to evaluate the posterior. Rather, we only require the mean $\\hat{\\mu} = \\frac{1}{N}\\sum_{n=1}^N x_n$ and $N$. These are called **sufficient statistics**.\n",
    "\n",
    "**Understanding the likelihood**\n",
    "\n",
    "The posterior is a distribution over $\\lambda$. This means that in order to understand its behaviour we need to understand how $p( x_n | \\lambda )$ behaves as a function of $\\lambda$. Let's make a familiar plot of the density of $x$ given $\\lambda$ that is $p( x | \\lambda )$ as a function of $x$ for three different settings of the decay constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(5, 50, 500) # values of x at which to evaluate the likelihood\n",
    "colors = ['red', 'green', 'blue'] # color of each line\n",
    "\n",
    "# some arrays to handle the probability going to zero outside the detector i.e. x<5 or x>50\n",
    "x_start = np.array([0, 5])\n",
    "x_end = np.array([50, 55])\n",
    "lik_0 = np.array([0, 0])\n",
    " \n",
    "for i, lamb in enumerate([5, 10, 20]): # at each step enumerate does: i, lamb = number_of_iteration, list_entry\n",
    "    Z = lamb*(np.exp(-5/lamb) - np.exp(-50/lamb)) # different Z for each lambda\n",
    "    lik = np.exp(-xs/lamb)/Z # evaluate the likelihood for all xs at once (Numpy is very handy)\n",
    "    \n",
    "    xs_wide = np.concatenate([x_start, xs,x_end], axis=0)\n",
    "    lik_wide = np.concatenate([lik_0, lik,lik_0], axis=0)\n",
    "    \n",
    "    plt.plot(xs_wide, lik_wide, color = colors[i], label = '$\\lambda$ = {}'.format(lamb)) # plot\n",
    "    beautify_plot({\"title\":\"Decay probability densities for different $\\lambda$\", \"x\":\"$x$\", \"y\":'$p(x|\\lambda)$'})\n",
    "\n",
    "\n",
    "plt.plot(np.array([15, 15]),np.array([0, 0.2]),'black')    \n",
    "plt.plot(np.array([15]),np.array([-0.01]),'red',marker = 'x')    \n",
    "\n",
    "\n",
    "plt.gca().legend(fontsize = 14) # add legend for maximum style\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "Now consider a decay event at x=15. This event has lowest density under $\\lambda = 5$: the density decays away quickly before this point. It has higher density under $\\lambda = 20$ as this function decays more slowly, but the larger normalising constant $Z(\\lambda)$ in this case limits the improvement. The event has highest density under $\\lambda=10$ which trades off these two effects.\n",
    "\n",
    "We are now in a postion to plot $p( x=15 | \\lambda )$ as a function of $\\lambda$, that is the likelihood of $\\lambda$ for this data-point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "lamb = np.linspace(0.1, 100, 500) # values of x at which to evaluate the likelihood\n",
    "xobs = 15 # observed value of x\n",
    "\n",
    "Z = lamb*(np.exp(-5/lamb) - np.exp(-50/lamb)) # different Z for each lambda\n",
    "lik = np.exp(-xobs/lamb)/Z # evaluate the likelihood for all xs at once (Numpy is very handy)\n",
    "    \n",
    "plt.plot(lamb, lik, color = 'blue') # plot\n",
    "beautify_plot({\"title\":\"Likelihood of $\\lambda$\", \"x\":\"$\\lambda$\", \"y\":'$p(x=15|\\lambda)$'})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "The likelihood as a peak around $\\lambda=15$ which is a result of the trade-off identified above between the speed of the decay and the size of the normlizing constant. As a result, in the posterior, this data point contributes weak evidence that the underlying vaue of $\\lambda$ is close to $15$. It rules out very small settings of $\\lambda$ (the decay event was seen at too large a value of $x$ to be consistent with a short decay constant), but it is ambivalent toward higher values of $\\lambda$ (afterall, in the limit of large $\\lambda$  we are equally likely to see the decay event at any location within the device so _any_ single decay cannot rule out large decay constants). The posterior contains a likelihood contribution of this sort from each datapoint which multiply together. \n",
    "\n",
    "To visualise the posterior we must also make a choice for the prior. Here we make the very simple assumption that before seeing data $\\lambda$ is uniformly likely to take a value from $0$ to $100$ and that it cannot take a value larger than this. That is, it obeys a uniform distribution,\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\lambda) = \\mathcal{U}(\\lambda; 0, 100)\n",
    "\\end{align}$$\n",
    "\n",
    "We will take in more detail later about how to select priors, but it is not the focus here.\n",
    "\n",
    "We can now plot the posterior distribution for our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0.1, 100, 1000) # Values of lambda where we want to evaluate the posterior.\n",
    "Z = lambdas*(np.exp(-5/lambdas) - np.exp(-50/lambdas))\n",
    "\n",
    "lik = np.exp(-np.sum(x)/lambdas)/Z**x.shape[0] # likelihood of the whole dataset\n",
    "\n",
    "prior = np.heaviside(lambdas.max()-lambdas, 0)/(lambdas.max()-lambdas.min()) # uniform prior [0, 100]\n",
    "\n",
    "post = lik*prior # calculate posterior (un-normalized)\n",
    "dlamda = ((lambdas.max()-lambdas.min())/lambdas.shape[0]) # length of small line element dlamda\n",
    "post /= post.sum()*dlamda # normalize posterior\n",
    "\n",
    "plt.plot(lambdas, post, color = 'black', zorder = 1) # plot posterior\n",
    "plt.scatter(lambdas[np.argmax(post)], post.max(), color='red', linewidth=1.5,\n",
    "            edgecolor='black', zorder = 2) # mark the posterior maximum with red point\n",
    "beautify_plot({\"title\":\"Posterior distribution\", \"x\":\"$\\lambda$\", \"y\":'$p(\\lambda \\mid \\{x\\})$'})\n",
    "plt.ylim([0, 0.02])\n",
    "plt.yticks(np.linspace(0, 0.02, 5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"radio_post.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This posterior has a peak around the value $\\lambda =22.3$, but the peak is quite weak with e.g. $\\lambda = 100$ having around half the probability density of the peak. This shows that there is weak evidence in the data that the value of $\\lambda$ is around $22$, but that there is considerable uncertainty.\n",
    "\n",
    "The posterior distribution is the ultimate answer to any inference problem encapsulating all the information we have about the parameters (both from the data and prior knowledge). However, the posterior distribution is a complex function and it is often useful to summarize it. \n",
    "\n",
    "One common approach is to compute mean and variance of $\\lambda$ under the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "lamda_mean = (post*lambdas*dlamda).sum() # calculate the mean lamda by integration\n",
    "lamda_std = (((lambdas - lamda_mean)**2)*post*dlamda).sum()**0.5 # calculate the lamda also by integration\n",
    "print('lambda mean: {}, lambda standard deviation (uncertainty): {}'.format(lamda_mean.round(2), lamda_std.round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "We could instead summarise our knowledge about $\\lambda$  using the mode of the posterior, which is known as the maximum a posteriori (MAP) value of $\\lambda$. The RMS variation around the MAP value can be used as error bars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "lamda_max = lambdas[np.argmax(post)] # use the maximum-posterior lamda as our estimate for lambda\n",
    "lamda_std = (((lambdas - lamda_max)**2)*post*dlamda).sum()**0.5 # calculate the lamda also by integration\n",
    "print('lambda mode: {}, lambda uncertainty: {}'.format(lamda_max.round(2), lamda_std.round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the MAP estimate gives a smaller $\\lambda$ than using the mean of $\\lambda$ since more weight falls to the right than to the left of the maximum of the posterior distribution (the distribution is skewed). \n",
    "\n",
    "-- perhaps add here a plot showing the mean, mode, and the two sets of error bars so they can be easily visualized --\n",
    "\n",
    "An alternative way of summarising the posterior to draw samples for $\\lambda$ from the posterior, $\\lambda \\sim p(\\lambda | \\{ x_n\\}_{n=1}^N)$. These samples can be thought of as 'typical values of the decay constant that are consistent with the observed data'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 4))\n",
    "\n",
    "colors = ['red', 'green', 'blue','magenta'] # color of each line\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "for n in range(4):\n",
    "    \n",
    "    lambda_sample = np.random.choice(lambdas, p = post/post.sum()) # random choice from lambdas with probabilities p\n",
    "    lik_sample = np.exp(-xs/lambda_sample)/lambda_sample # single-sample likelihood for sampled lambda\n",
    "    plt.scatter(lambda_sample, 0, marker = 'o', linewidth = 1.5,\n",
    "                edgecolor = 'black', zorder = 3, color = colors[n], clip_on = False) # plot point\n",
    "    \n",
    "plt.ylim([0, 0.06])\n",
    "plt.plot(lambdas, post, color = 'black', zorder = 1)\n",
    "beautify_plot({\"title\":\"Posterior and samples\", \"x\":\"$\\lambda$\", \"y\":\"$p(\\lambda \\mid \\{x_n\\}_{n=1}^N)$\"})\n",
    "plt.ylim([0, 0.02])\n",
    "plt.yticks(np.linspace(0, 0.02, 5))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "for n in range(4):\n",
    "    \n",
    "    lambda_sample = np.random.choice(lambdas, p = post/post.sum()) # random choice from lambdas with probabilities p\n",
    "    lik_sample = np.exp(-xs/lambda_sample)/lambda_sample # single-sample likelihood for sampled lambda\n",
    "    plt.plot(xs, lik_sample, color = colors[n]) # plot likelihood\n",
    "    \n",
    "\n",
    "beautify_plot({\"title\":\"decay probabilities\", \"x\":\"$x^\\star$\", \"y\":\"$p(x^\\star \\mid \\lambda)$\"})\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end this introductory example by considering one last question:\n",
    "\n",
    "**How should we predict where the next decay event will occur?**\n",
    "\n",
    "Prediction is a fundamental part of science and engineering. The probabilistic approach says:\n",
    "\n",
    "1. that the answer to this question is the probability of the location next decay event $x^\\star$ given the observed data \\{x_n\\}_{n=1}^N, that is $p(x^\\star \\lvert \\{x_n\\}_{n=1}^N)$. This is known as the predictive distribution.\n",
    "\n",
    "2. that the predictive distribution can be computed using the rules of probability\n",
    "\n",
    "\\begin{align}\n",
    "p(x^\\star \\lvert \\{x_n\\}_{n=1}^N) &= \\int p(x^\\star,\\lambda \\lvert \\{x_n\\}_{n=1}^N) \\text{d} \\lambda \\;\\;\\; \\text{(sum rule)}\\\\\n",
    "%\n",
    "& = \\int p(x^\\star \\lvert  \\lambda, \\{x_n\\}_{n=1}^N) p(\\lambda | \\{x_n\\}_{n=1}^N) \\text{d} \\lambda \\;\\;\\; \\text{(product rule)}\\\\ \n",
    "%\n",
    "& = \\int p(x^\\star \\lvert  \\lambda) p(\\lambda | \\{x_n\\}_{n=1}^N) \\text{d} \\lambda \\;\\;\\; \\text{(modelling assumptions)}\\\\ \n",
    "\\end{align}\n",
    "\n",
    "The last line follows from that fact that if we know $\\lambda$ then, under the model we are assuming, the data set $\\{x_n\\}_{n=1}^N$ provides no additional information about $x^\\star $.\n",
    "\n",
    "The predictive distribution has an intuitive form: it takes the prediction we would make if we knew $\\lambda$, $p(x^\\star \\lvert  \\lambda)$, weights it by the probability under the posterior $p(\\lambda | \\{x_n\\}_{n=1}^N)$, and sums this quatity over all settings of $\\lambda$.\n",
    "\n",
    "Here's the predictive distribution in our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0.1, 100, 1000)\n",
    "xs = np.linspace(5, 50, 200)\n",
    "l_grid, x_grid = np.meshgrid(lambdas, xs)\n",
    "\n",
    "Z = l_grid*(np.exp(-5/l_grid) - np.exp(-50/l_grid)) # different Z for each lambda\n",
    "\n",
    "\n",
    "lik = np.exp(-x_grid/l_grid)/Z\n",
    "\n",
    "integrand = lik*post\n",
    "dlambda = (lambdas.max() - lambdas.min())/lambdas.shape[0]\n",
    "predictive = integrand.sum(axis = 1)*dlambda\n",
    "\n",
    "# MAP prediction\n",
    "ZMAP = lamda_max*(np.exp(-5/lamda_max) - np.exp(-50/lamda_max)) # different Z for each lambda\n",
    "predictive_MAP = np.exp(-xs/lamda_max)/ZMAP;\n",
    "\n",
    "# handle regions outside of detector \n",
    "x_start = np.array([0, 5])\n",
    "x_end = np.array([50, 55])\n",
    "predictive_0 = np.array([0, 0])\n",
    "xs_wide = np.concatenate([x_start, xs,x_end], axis=0)\n",
    "predictive_wide = np.concatenate([predictive_0, predictive,predictive_0], axis=0)\n",
    "predictive_MAP_wide = np.concatenate([predictive_0, predictive_MAP,predictive_0], axis=0)\n",
    "\n",
    "\n",
    "plt.plot(xs_wide, predictive_MAP_wide, color = 'red', label = '$p(x^\\star | \\lambda_{\\mathrm{MAP}} )$')\n",
    "plt.plot(xs_wide, predictive_wide, color = 'black', label = '$p(x^\\star | \\{x_n\\}_{n=1}^N)$')\n",
    "\n",
    " \n",
    "\n",
    "beautify_plot({\"title\":\"Predictive distribution\", \"x\":\"$x^\\star$\", \"y\":\"$p(x^\\star|\\{x\\})$\"})\n",
    "plt.xlim([0, 55])\n",
    "plt.ylim([-0.001, 0.055])\n",
    "plt.yticks(np.linspace(0, 0.05, 5))\n",
    "plt.tight_layout()\n",
    "plt.gca().legend(fontsize = 14) # add legend for maximum style\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "Here we have compared the predictive distribution to the prediction made by the MAP setting of the parameters. The predictive is less confident. Can you explain why?\n",
    "\n",
    "Again, the predictive distribition can be summarised e.g. by a mean and a standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = (xs.max() - xs.min())/xs.shape[0]\n",
    "x_mean = (predictive*xs*dx).sum()\n",
    "x_std = (predictive*(xs - x_mean)**2).sum()**0.5\n",
    "\n",
    "print(\"Mean x: {}, standard deviation (uncertainty): {}\".format(x_mean.round(2), x_std.round(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth taking a moment to consider what we have done above:\n",
    "\n",
    "1. Assumed a form for the probabilistic relationship between the underlying parameter and each data point $p(\\x_n|\\lambda)$.\n",
    "2. Encoded prior information we have about $\\lambda$ before seeing data into a **uniform prior distribution $p(\\lambda)$**.\n",
    "3. Applied Bayes' rule to find the posterior $p(\\lambda|\\{x_n\\}_{n=1}^N)$ which told us how plausible each value of $\\lambda$ is after observing the data set.\n",
    "4. Calculated the predictive distribution of new decay events given the observed data $p(x^\\star | \\{x_{n}\\}_{n=1}^N)$.\n",
    "\n",
    "The approach is much more explicit and principled than the two heuristic methods proposed to begin with. Such approaches which retain probability distributions over parameters are called **Bayesian**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".jupyter-widgets-disconnected::before{\n",
    "    visibility: hidden;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02a2429d4132422490637e8dab25248b": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_e5ad36bfbecd4ae8939e35af3331b659",
       "outputs": [
        {
         "data": {
          "text/latex": "Since $p(x|\\lambda)$ is a probability distribution in $x$, it must be normalized:\n$$\\\\$$\n$$\n\\int^{x_{max}}_{x_{min}} p(x|\\lambda) dx = \\frac{1}{Z(\\lambda)}\\int^{x_{max}}_{x_{min}} \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = 1\\\\\n$$\n$$\\\\$$\n$$\n\\implies Z(\\lambda) = \\int^{x_{max}}_{x_{min}} \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = \\lambda \\bigg[\\text{exp}\\bigg(-\\frac{x_{min}}{\\lambda}\\bigg) - \\text{exp}\\bigg(-\\frac{x_{max}}{\\lambda}\\bigg) \\bigg]\\\\\n$$\n$$\\\\$$",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "02afc223e3d049a388e5c1021644e393": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_27c658e55d00485b9eb9085655e927c4",
       "outputs": [
        {
         "data": {
          "text/latex": "Since $p(x|\\lambda)$ is a probability distribution in $x$, it must be normalized:\n\n\\begin{align}\n\\int^\\infty_0 p(x|\\lambda) dx &= \\frac{1}{Z(\\lambda)}\\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = 1\\\\\n~\\\\\n\\implies Z(\\lambda) &= \\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = \\lambda\\\\\n\\end{align}\n",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "06122dc121194c89a4149a6ad9b2faa5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "082efebfa05848f7b454624cdd0924ce": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_735c23d8472043c1a386193b5d7e0c94",
       "outputs": [
        {
         "ename": "NameError",
         "evalue": "name 'file' is not defined",
         "output_type": "error",
         "traceback": [
          "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
          "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
          "\u001b[0;32m<ipython-input-4-f82797e70253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLatex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
          "\u001b[0;31mNameError\u001b[0m: name 'file' is not defined"
         ]
        }
       ]
      }
     },
     "0901ad434ef24e059e245e79e7b3c875": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0ac4bbc49a7a45cdb28ffdb1aaa1ea7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "0cb35c3e7d7a43ea929e03a3c38315de": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_f20874a740494b709d84c672367abe0e",
       "outputs": [
        {
         "data": {
          "text/latex": "$$$$",
          "text/plain": "<IPython.core.display.Math object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "0e5e7f6adafe4715a4e96eefff6b36ae": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_c7018de66c8e40a0871aba322d6199f3",
       "outputs": [
        {
         "data": {
          "text/latex": "Since the decays are all independent from each other, the conditional probabilities of each $x_i$ simplify\n\n$$\np(x_i|\\{x\\}_{i\\neq j}) = p(x_i)\n$$\n\nThen, using Bayes’ rule\n\n\\begin{align}\np(x_1 \\cap x_2 \\cap … x_N) &= p(x_1 | x_2 \\cap x_3 … x_N) p(x_2 \\cap x_3 … x_N)\\\\\n~\\\\\n&= p(x_1)p(x_2 \\cap x_3 … x_N)\\\\\n~\\\\\n&=\\cdots\\\\\n~\\\\\n&= \\prod^N_{n = 1} p(x_n)~~~\\text{by induction}\\\\\n~\\\\\n&= \\frac{1}{\\lambda^N} \\text{exp}\\bigg(\\frac{\\sum^N_{n = 1} x}{\\lambda}\\bigg)\n\\end{align}",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "109e3c8e925d417399a2225c5a0583dc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "12b48824652d44e5bd4c46a8edb45dea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1ce9b921e5ec44e5812dd97d89d535f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "The value of Z"
       },
       "children": [
        "IPY_MODEL_02a2429d4132422490637e8dab25248b"
       ],
       "layout": "IPY_MODEL_4b118631c1af49a0ac40cfe015b7a843",
       "selected_index": null
      }
     },
     "1d8d143e91c54ded923778511d499529": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "The value of Z"
       },
       "children": [
        "IPY_MODEL_21257b55d12b40d598f477fbb1bec343"
       ],
       "layout": "IPY_MODEL_266169d378ed44bebf4cf9c7c6c4c074",
       "selected_index": null
      }
     },
     "1eca8b385f0746e9ab9083f07e1b18b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "21257b55d12b40d598f477fbb1bec343": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_fb3c80950bc2498bababb7bb5427a805",
       "outputs": [
        {
         "data": {
          "text/latex": "Since $p(x|\\lambda)$ is a probability distribution in $x$, it must be normalized:\n\n\\begin{align}\n\\int^\\infty_0 p(x|\\lambda) dx &= \\frac{1}{Z(\\lambda)}\\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = 1\\\\\n~\\\\\n\\implies Z(\\lambda) &= \\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = \\lambda\\\\\n\\end{align}\n",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "233e36c8b12d4be0ac458239def0ce07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "24a8f31ebbf34ebf963eeaced5b73235": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "266169d378ed44bebf4cf9c7c6c4c074": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "27c658e55d00485b9eb9085655e927c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "37732983e6fa445cb0d111ba79a65a68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "The value of Z"
       },
       "children": [
        "IPY_MODEL_a3341d973c5d40d5a1279acd81236604"
       ],
       "layout": "IPY_MODEL_b5d5f8bf51434599b5fe58ad63821145",
       "selected_index": null
      }
     },
     "3c97a668ee5745d3be6070de57fd4680": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "An informal proof of Bayes' rule"
       },
       "children": [
        "IPY_MODEL_53a7efd2ac0a4e5e8a2983eb8ce1266e"
       ],
       "layout": "IPY_MODEL_ec6d36284ae5471e82aa6ca4983eac18",
       "selected_index": null
      }
     },
     "4338fe0dac00440da12a7a285d37db76": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "The value of Z"
       },
       "children": [
        "IPY_MODEL_54e50e3c9e82494d9af0c3a16d2b8468"
       ],
       "layout": "IPY_MODEL_0ac4bbc49a7a45cdb28ffdb1aaa1ea7d",
       "selected_index": null
      }
     },
     "48f023f3422a440880490d524ccef7ef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "49440362558041fbb3142a96b7c54f40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "The value of Z"
       },
       "children": [
        "IPY_MODEL_611b0203a14144a682f44881271d4f61"
       ],
       "layout": "IPY_MODEL_0901ad434ef24e059e245e79e7b3c875",
       "selected_index": null
      }
     },
     "4b118631c1af49a0ac40cfe015b7a843": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4da9bef8da794909ae3b1ff8181d8681": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5024b8cac6fe4073824ec9483f10d830": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "Likelihood in more detail"
       },
       "children": [
        "IPY_MODEL_f664fe2028e8455194d14b63a0e900a2"
       ],
       "layout": "IPY_MODEL_24a8f31ebbf34ebf963eeaced5b73235",
       "selected_index": null
      }
     },
     "53a7efd2ac0a4e5e8a2983eb8ce1266e": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_d67706ec43bf4b4ba5077deca4f6e137",
       "outputs": [
        {
         "data": {
          "text/latex": "We want to show that for two events A and B\n\n\\begin{align}\np(A|B)p(B) = p(A \\cap B) = p(B|A)P(A).\n\\end{align}\n\nIf we showed the above, then Bayes’ rule comes out instantly:\n\n\\begin{align}\np(A|B) = \\frac{p(A|B)P(B)}{p(A)}.\n\\end{align}\n\nThus we would like to provide some justification as to why $p(A|B)p(B) = p(A \\cap B)$ holds. The conditional probability $p(A|B)$ denotes “the probability of A occurring given B has occurred”, or equivalently “the fraction of outcomes for which A is true, out of all possible outcomes for which B is true”. Let $n_{ab} \\in \\{n_{00}, n_{01}, n_{10}, n_{11}\\}$ denote the number of (equiprobable) outcomes for which A is true/false and B is true/false depending on the indices: e.g. $n_{01}$ denotes the number of outcomes for which A is false and B is true. Then defining $N = n_{00} + n_{01} + n_{10} + n_{11}$\n\n\\begin{align}\np(A|B) = \\frac{n_{11}}{n_{01} + n_{11}} = \\frac{\\frac{n_{11}}{N}}{\\frac{n_{01} + n_{11}}{N}} = \\frac{p(A \\cap B)}{p(B)}\n\\end{align}\n\nthrough which we obtain the required result from which Bayes’ rule follows. This can readily be extended to situations where more than two events are considered by including more indices.",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "5497c3730f364695b1323c57ca63ee0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "An informal proof of Bayes' rule"
       },
       "children": [
        "IPY_MODEL_876d75f896f6493c85c6d8efce9b0756"
       ],
       "layout": "IPY_MODEL_06122dc121194c89a4149a6ad9b2faa5",
       "selected_index": null
      }
     },
     "54e50e3c9e82494d9af0c3a16d2b8468": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_e7b32aba550c4f6b8bf949720bdb5bc1",
       "outputs": [
        {
         "data": {
          "text/latex": "Since $p(x|\\lambda)$ is a probability distribution in $x$, it must be normalized:\n$$\\\\$$\n$$\n\\int^{x_{max}}_{x_{min}} p(x|\\lambda) dx = \\frac{1}{Z(\\lambda)}\\int^{x_{max}}_{x_{min}} \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = 1\\\\\n$$\n$$\\\\$$\n$$\n\\implies Z(\\lambda) = \\int^{x_{max}}_{x_{min}} \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = \\lambda \\bigg[\\text{exp}\\bigg(-\\frac{x_{min}}{\\lambda}\\bigg) - \\text{exp}\\bigg(-\\frac{x_{max}}{\\lambda}\\bigg) \\bigg]\\\\\n$$\n$$\\\\$$",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "5a5750fd0ab14fc9b994d57e93b04f04": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6094c3712c2340baaeb62eec469e3b01": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "611b0203a14144a682f44881271d4f61": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_c1f6915f2bf64949a8265a2d09080ef7",
       "outputs": [
        {
         "data": {
          "text/latex": "Since $p(x|\\lambda)$ is a probability distribution in $x$, it must be normalized:\n\n\\begin{align}\n\\int^\\infty_0 p(x|\\lambda) dx &= \\frac{1}{Z(\\lambda)}\\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = 1\\\\\n~\\\\\n\\implies Z(\\lambda) &= \\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = \\lambda\\\\\n\\end{align}\n",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "64cd379ec2764f608e58dc6571f835d4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "654bbc783feb4b5d80089eaea0f5c678": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "An informal proof of Bayes' rule"
       },
       "children": [
        "IPY_MODEL_7b3c6636b1b5420a8eadbf9471160c08"
       ],
       "layout": "IPY_MODEL_64cd379ec2764f608e58dc6571f835d4",
       "selected_index": null
      }
     },
     "6d81b3553bbc43f08c78a91f5ca6d075": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "70b5486d233c45ac9797c6be419b9d96": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_ab3ae54e61674b45940d05ce32bc1426",
       "outputs": [
        {
         "data": {
          "text/latex": "Since the decays are all independent from each other, the conditional probabilities of each $x_i$ simplify\n\n$$\np(x_i|\\{x\\}_{i\\neq j}) = p(x_i)\n$$\n\nThen, using Bayes’ rule\n\n\\begin{align}\np(x_1 \\cap x_2 \\cap … x_N) &= p(x_1 | x_2 \\cap x_3 … x_N) p(x_2 \\cap x_3 … x_N)\\\\\n~\\\\\n&= p(x_1)p(x_2 \\cap x_3 … x_N)\\\\\n~\\\\\n&=\\cdots\\\\\n~\\\\\n&= \\prod^N_{n = 1} p(x_n)~~~\\text{by induction}\\\\\n~\\\\\n&= \\frac{1}{\\lambda^N} \\text{exp}\\bigg(\\frac{\\sum^N_{n = 1} x}{\\lambda}\\bigg)\n\\end{align}",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "735c23d8472043c1a386193b5d7e0c94": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7981bdf518ea4c3992a8bb647b221ec9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7b3c6636b1b5420a8eadbf9471160c08": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_a8c810e4ee4f48acb8638818f2f852ae",
       "outputs": [
        {
         "data": {
          "text/latex": "We want to show that for two events A and B\n\n$$\\\\$$\n$$\np(A|B)p(B) = p(A, B) = p(B|A)P(A).\\\\\n$$\n$$\\\\$$\n\nIf we showed the above, then Bayes’ rule comes out instantly:\n\n$$\\\\$$\n$$\np(A|B) = \\frac{p(A|B)P(B)}{p(A)}.\\\\\n$$\n$$\\\\$$\n\nThus we would like to provide some justification as to why $p(A|B)p(B) = p(A, B)$ holds. The conditional probability $p(A|B)$ denotes “the probability of A occurring given B has occurred”, or equivalently “the fraction of outcomes for which A is true, out of all possible outcomes for which B is true”. Let $n_{ab} \\in \\{n_{00}, n_{01}, n_{10}, n_{11}\\}$ denote the number of (equiprobable) outcomes for which A is true/false and B is true/false depending on the indices: e.g. $n_{01}$ denotes the number of outcomes for which A is false and B is true. Then defining $N = n_{00} + n_{01} + n_{10} + n_{11}$\n\n$$\\\\$$\n$$\np(A|B) = \\frac{n_{11}}{n_{01} + n_{11}} = \\frac{\\frac{n_{11}}{N}}{\\frac{n_{01} + n_{11}}{N}} = \\frac{p(A, B)}{p(B)}\\\\\n$$\n$$\\\\$$\n\nthrough which we obtain the required result from which Bayes’ rule follows. This can readily be extended to situations where more than two events are considered by including more indices.",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "876d75f896f6493c85c6d8efce9b0756": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_7981bdf518ea4c3992a8bb647b221ec9",
       "outputs": [
        {
         "data": {
          "text/latex": "We want to show that for two events A and B\n\n$$\\\\$$\n$$\np(A|B)p(B) = p(A, B) = p(B|A)P(A).\\\\\n$$\n$$\\\\$$\n\nIf we showed the above, then Bayes’ rule comes out instantly:\n\n$$\\\\$$\n$$\np(A|B) = \\frac{p(A|B)P(B)}{p(A)}.\\\\\n$$\n$$\\\\$$\n\nThus we would like to provide some justification as to why $p(A|B)p(B) = p(A, B)$ holds. The conditional probability $p(A|B)$ denotes “the probability of A occurring given B has occurred”, or equivalently “the fraction of outcomes for which A is true, out of all possible outcomes for which B is true”. Let $n_{ab} \\in \\{n_{00}, n_{01}, n_{10}, n_{11}\\}$ denote the number of (equiprobable) outcomes for which A is true/false and B is true/false depending on the indices: e.g. $n_{01}$ denotes the number of outcomes for which A is false and B is true. Then defining $N = n_{00} + n_{01} + n_{10} + n_{11}$\n\n$$\\\\$$\n$$\np(A|B) = \\frac{n_{11}}{n_{01} + n_{11}} = \\frac{\\frac{n_{11}}{N}}{\\frac{n_{01} + n_{11}}{N}} = \\frac{p(A, B)}{p(B)}\\\\\n$$\n$$\\\\$$\n\nthrough which we obtain the required result from which Bayes’ rule follows. This can readily be extended to situations where more than two events are considered by including more indices.",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "89583625be524befa28717068cc4b65a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8da4249fec554e0e8952482d683421c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "The value of Z"
       },
       "children": [
        "IPY_MODEL_b40c719b12b6447dbec0a134eec6ddf6"
       ],
       "layout": "IPY_MODEL_c5c7113aa40c4e6ea6472c47b61ecdfc",
       "selected_index": null
      }
     },
     "9fa1d113712849549c1e8eb1aedcb048": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "children": [
        "IPY_MODEL_d8249534d98e40ff937246668f8904f2"
       ],
       "layout": "IPY_MODEL_109e3c8e925d417399a2225c5a0583dc"
      }
     },
     "a3341d973c5d40d5a1279acd81236604": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_89583625be524befa28717068cc4b65a",
       "outputs": [
        {
         "data": {
          "text/latex": "Since $p(x|\\lambda)$ is a probability distribution in $x$, it must be normalized:\n\n\\begin{align}\n\\int^\\infty_0 p(x|\\lambda) dx &= \\frac{1}{Z(\\lambda)}\\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = 1\\\\\n~\\\\\n\\implies Z(\\lambda) &= \\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = \\lambda\\\\\n\\end{align}\n",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "a3929f58913e4d12bb5b585b72bac4ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "The value of Z"
       },
       "children": [
        "IPY_MODEL_0cb35c3e7d7a43ea929e03a3c38315de"
       ],
       "layout": "IPY_MODEL_48f023f3422a440880490d524ccef7ef",
       "selected_index": null
      }
     },
     "a8c810e4ee4f48acb8638818f2f852ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aa71db7f45e64fc389d7aab677d0c747": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "An informal proof of Bayes' rule"
       },
       "children": [
        "IPY_MODEL_c03d2206e597483c9094681c0ffe5c4b"
       ],
       "layout": "IPY_MODEL_bb2d295a10ef4694baa109a1142346c5",
       "selected_index": null
      }
     },
     "ab3ae54e61674b45940d05ce32bc1426": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ad595a43cdd148eb9e1539c1c4ee8ee9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "The value of Z"
       },
       "children": [
        "IPY_MODEL_02afc223e3d049a388e5c1021644e393"
       ],
       "layout": "IPY_MODEL_b2ac5641b30f40be875b1087ce4bf1f6"
      }
     },
     "b2ac5641b30f40be875b1087ce4bf1f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b40c719b12b6447dbec0a134eec6ddf6": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_6d81b3553bbc43f08c78a91f5ca6d075",
       "outputs": [
        {
         "data": {
          "text/latex": "Since $p(x|\\lambda)$ is a probability distribution in $x$, it must be normalized:\n\n\\begin{align}\n\\int^\\infty_0 p(x|\\lambda) dx &= \\frac{1}{Z(\\lambda)}\\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = 1\\\\\n~\\\\\n\\implies Z(\\lambda) &= \\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = \\lambda\\\\\n\\end{align}\n",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "b5d5f8bf51434599b5fe58ad63821145": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b92d600645c14d328c20a85c99605c94": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "Likelihood in more detail"
       },
       "children": [
        "IPY_MODEL_70b5486d233c45ac9797c6be419b9d96"
       ],
       "layout": "IPY_MODEL_c00810ca721c4cd38db02051cf24d9d1"
      }
     },
     "bb2d295a10ef4694baa109a1142346c5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c00810ca721c4cd38db02051cf24d9d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c03d2206e597483c9094681c0ffe5c4b": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_233e36c8b12d4be0ac458239def0ce07",
       "outputs": [
        {
         "data": {
          "text/latex": "We want to show that for two events A and B\n\n\\begin{align}\np(A|B)p(B) = p(A \\cap B) = p(B|A)P(A).\n\\end{align}\n\nIf we showed the above, then Bayes’ rule comes out instantly:\n\n\\begin{align}\np(A|B) = \\frac{p(A|B)P(B)}{p(A)}.\n\\end{align}\n\nThus we would like to provide some justification as to why $p(A|B)p(B) = p(A \\cap B)$ holds. The conditional probability $p(A|B)$ denotes “the probability of A occurring given B has occurred”, or equivalently “the fraction of outcomes for which A is true, out of all possible outcomes for which B is true”. Let $n_{ab} \\in \\{n_{00}, n_{01}, n_{10}, n_{11}\\}$ denote the number of (equiprobable) outcomes for which A is true/false and B is true/false depending on the indices: e.g. $n_{01}$ denotes the number of outcomes for which A is false and B is true. Then defining $N = n_{00} + n_{01} + n_{10} + n_{11}$\n\n\\begin{align}\np(A|B) = \\frac{n_{11}}{n_{01} + n_{11}} = \\frac{\\frac{n_{11}}{N}}{\\frac{n_{01} + n_{11}}{N}} = \\frac{p(A \\cap B)}{p(B)}\n\\end{align}\n\nthrough which we obtain the required result from which Bayes’ rule follows. This can readily be extended to situations where more than two events are considered by including more indices.",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "c1f6915f2bf64949a8265a2d09080ef7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c4002b4c197c4ae4962377c979daf18a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "children": [
        "IPY_MODEL_e39821cd7d624bbbb0fa970e6d1efcd1"
       ],
       "layout": "IPY_MODEL_12b48824652d44e5bd4c46a8edb45dea"
      }
     },
     "c5c7113aa40c4e6ea6472c47b61ecdfc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c68c635c59b24f5dab19dd6bf7e84ce0": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_f5bb0efbfdd940428a19529f58ce1544",
       "outputs": [
        {
         "data": {
          "text/latex": "Since the decays are all independent from each other, the conditional probabilities of each $x_i$ simplify\n\n$$\np(x_i|\\{x\\}_{i\\neq j}) = p(x_i)\n$$\n\nThen, using Bayes’ rule\n\n\\begin{align}\np(x_1 \\cap x_2 \\cap … x_N) &= p(x_1 | x_2 \\cap x_3 … x_N) p(x_2 \\cap x_3 … x_N)\\\\\n~\\\\\n&= p(x_1)p(x_2 \\cap x_3 … x_N)\\\\\n~\\\\\n&=\\cdots\\\\\n~\\\\\n&= \\prod^N_{n = 1} p(x_n)~~~\\text{by induction}\\\\\n~\\\\\n&= \\frac{1}{\\lambda^N} \\text{exp}\\bigg(\\frac{\\sum^N_{n = 1} x}{\\lambda}\\bigg)\n\\end{align}",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "c7018de66c8e40a0871aba322d6199f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d67706ec43bf4b4ba5077deca4f6e137": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d8249534d98e40ff937246668f8904f2": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_4da9bef8da794909ae3b1ff8181d8681",
       "outputs": [
        {
         "ename": "NameError",
         "evalue": "name 'file' is not defined",
         "output_type": "error",
         "traceback": [
          "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
          "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
          "\u001b[0;32m<ipython-input-2-f82797e70253>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLatex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
          "\u001b[0;31mNameError\u001b[0m: name 'file' is not defined"
         ]
        }
       ]
      }
     },
     "e111500203e14e40b6495cda731ceffa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e39821cd7d624bbbb0fa970e6d1efcd1": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_6094c3712c2340baaeb62eec469e3b01",
       "outputs": [
        {
         "data": {
          "text/latex": "Since $p(x|\\lambda)$ is a probability distribution in $x$, it must be normalized:\n\n\\begin{align}\n\\int^\\infty_0 p(x|\\lambda) dx &= \\frac{1}{Z(\\lambda)}\\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = 1\\\\\n~\\\\\n\\implies Z(\\lambda) &= \\int^\\infty_0 \\text{exp}\\bigg(-\\frac{x}{\\lambda}\\bigg) dx = \\lambda\\\\\n\\end{align}\n",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "e39a06e8f9b2449fbbad78c760fd059b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "Likelihood in more detail"
       },
       "children": [
        "IPY_MODEL_c68c635c59b24f5dab19dd6bf7e84ce0"
       ],
       "layout": "IPY_MODEL_e111500203e14e40b6495cda731ceffa",
       "selected_index": null
      }
     },
     "e5ad36bfbecd4ae8939e35af3331b659": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e783f66ff2434ecc9352665385ca9872": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "Likelihood in more detail"
       },
       "children": [
        "IPY_MODEL_0e5e7f6adafe4715a4e96eefff6b36ae"
       ],
       "layout": "IPY_MODEL_5a5750fd0ab14fc9b994d57e93b04f04",
       "selected_index": null
      }
     },
     "e7b32aba550c4f6b8bf949720bdb5bc1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e939ab6f33064187844fb5439a456147": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.2.0",
      "model_name": "AccordionModel",
      "state": {
       "children": [
        "IPY_MODEL_082efebfa05848f7b454624cdd0924ce"
       ],
       "layout": "IPY_MODEL_f87d7e0397a240b28302f4e0fa23d697"
      }
     },
     "ec6d36284ae5471e82aa6ca4983eac18": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f20874a740494b709d84c672367abe0e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f5bb0efbfdd940428a19529f58ce1544": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f664fe2028e8455194d14b63a0e900a2": {
      "model_module": "@jupyter-widgets/output",
      "model_module_version": "1.0.0",
      "model_name": "OutputModel",
      "state": {
       "layout": "IPY_MODEL_1eca8b385f0746e9ab9083f07e1b18b5",
       "outputs": [
        {
         "data": {
          "text/latex": "Since the decays are all independent from each other, the conditional probabilities of each \\(x_i\\) simplify\n\n$$\n~\\\\\np(x_i|\\{x\\}_{i\\neq j}) = p(x_i)\\\\\n$$\n\nThen, using Bayes’ rule\n\n$$\n~\\\\\np(x_1, x_2, … x_N) = p(x_1 | x_2, x_3 … x_N) p(x_2, x_3 … x_N)\\\\\n~\\\\\n= p(x_1)p(x_2, x_3 … x_N)\\\\\n~\\\\\n=\\cdots\\\\\n~\\\\\n= \\prod^N_{n = 1} p(x_n)~~~\\text{by induction}\\\\\n~\\\\\n= \\frac{1}{Z(\\lambda)^N} \\text{exp}\\bigg(\\frac{\\sum^N_{n = 1} x_n}{\\lambda}\\bigg)\\\\\n$$",
          "text/plain": "<IPython.core.display.Latex object>"
         },
         "metadata": {},
         "output_type": "display_data"
        }
       ]
      }
     },
     "f87d7e0397a240b28302f4e0fa23d697": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fb3c80950bc2498bababb7bb5427a805": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
