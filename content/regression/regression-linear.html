
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Linear regression &#8212; OMLT</title>
    
  <link rel="stylesheet" href="../../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom_style.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/.ipynb_checkpoints/custom_style-checkpoint.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Regression" href="regression-intro.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../../index.html">
  
  <img src="../../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">OMLT</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../home.html">
   Online Machine Learning Textbook
  </a>
 </li>
</ul>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1 current active collapsible-parent">
  <a class="reference internal" href="regression-intro.html">
   Regression
  </a>
  <ul class="current collapse-ul">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear regression
    </a>
   </li>
  </ul>
  <i class="fas fa-chevron-down">
  </i>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/regression/regression-linear.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/stratisMarkou/random-walks/master?urlpath=tree/./content/regression/regression-linear.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#least-squares-fitting">
   Least Squares Fitting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-fitting">
   Maximum likelihood fitting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#questions">
   Questions
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="linear-regression">
<h1>Linear regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h1>
<p>This section will introduce <strong>linear regression</strong>, <strong>least squares fitting</strong>, and the relationship to <strong>probabilistic modelling</strong> and <strong>maximum likelihood estimation</strong>.</p>
<p>Let’s start by considering a toy dataset (shown below) consisting of <span class="math notranslate nohighlight">\(N=10\)</span> scalar input and output pairs <span class="math notranslate nohighlight">\(\{ x_{n}, y_{n} \}_{n=1}^N\)</span>. Our goal will be to make predictions at new input locations <span class="math notranslate nohighlight">\(x^\star\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Load some prepared data</span>
<span class="n">x_lin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;reg_lin_x.npy&#39;</span><span class="p">)</span>
<span class="n">y_lin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;reg_lin_y.npy&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_center-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Plot data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_lin</span><span class="p">,</span> <span class="n">y_lin</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="c1"># Format plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;A simple dataset&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/regression-linear_2_0.png" src="../../_images/regression-linear_2_0.png" />
</div>
</div>
<p>A simple and popular approach to regression is fit a linear function to the data and use this to make predictions at new input locations. Denoting the slope of the linear function <span class="math notranslate nohighlight">\(w_1\)</span> and intercept <span class="math notranslate nohighlight">\(w_0\)</span>, we have</p>
<p>\begin{align}
f(x) = w_1 x + w_0.
\end{align}</p>
<div class="section" id="least-squares-fitting">
<h2>Least Squares Fitting<a class="headerlink" href="#least-squares-fitting" title="Permalink to this headline">¶</a></h2>
<p>The task is now to estimate the parameters <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span> from the data. A straight line clearly can’t pass precisely through all of the datapoints because they aren’t collinear, but it is clear that some parameter choices will result in straight lines that go closer to data than others. A popular measure for the goodness of fit is the sum of square distances between each data point and <span class="math notranslate nohighlight">\(f(x_n)\)</span>,</p>
<p>\begin{align}
C_2 = \sum^N_{n = 1} \big[y_n - f(x_n) \big]^2  = \sum^N_{n = 1} \big[y_n - (w_1x_n + w_0)\big]^2 \geq 0
\end{align}</p>
<p>where equality with 0 holds only if \(~y_n = w_1x_n + w_0\) for every \(n\). By this measure, the optimal fit is the fit which minimises <span class="math notranslate nohighlight">\(C_2\)</span>. This cost function, although intuitive, lacks justification. We will return to this issue at the end of this section.</p>
<p>The following animation encapsulates the proposed approach problem. In the left plot, we visualise lines corresponding to different settings of the parameters <span class="math notranslate nohighlight">\((w_0, w_1)\)</span> in data space <span class="math notranslate nohighlight">\((x,y)\)</span>. The distances between the observed data and the fit are shown by grey dashes. The sum of the squares of these distances is <span class="math notranslate nohighlight">\(C_2\)</span>. The right plot shows contours of the cost <span class="math notranslate nohighlight">\(C_2\)</span> in parameter space with blue indicating the cost is low and red high. The black cross shows the parameters of the line displayed on the left. When the cross moves horizontally in parameter space the intercept of the line changes and when the cross moves vertically the slope changes.</p>
<!-- <div class="row">
  <div class="column">
    <img src="reg_lin_weight_excursion.gif" style="width:80%; float: center; padding: 0px">
  </div>
</div> -->
<p>Our goal is to find the parameters \((w_0, w_1)\) which minimise the cost. Since the cost has a simple form (it is a quadratic function of the parameters), this minimisation has an analytic solution. In order to derive that solution, we will first write the cost in a more compact and general form and second use differentiation to solve for the minimum.</p>
<p><strong>Step 1</strong>: writing \(C_2\) in a more convenient form</p>
<p>The cost can be written in terms of a  vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> containing the model parameters, a vector <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> that contains the training outputs, and a matrix <span class="math notranslate nohighlight">\(X\)</span> which contains the inputs augmented with a column of ones.</p>
<p>\[
C_2 = \big|\big|\mathbf{y} - \mathbf{X}\mathbf{w}\big|\big|^2 = \big(\mathbf{y} - \mathbf{X}\mathbf{w}\big)^\top \big(\mathbf{y} - \mathbf{X}\mathbf{w}\big)
\]</p>
<p>\begin{equation}
\text{where}~~~
\mathbf{y} = \begin{pmatrix}
y_1\<br />
y_2\<br />
\vdots \<br />
y_N
\end{pmatrix}, ~~~
\mathbf{X} =  \begin{pmatrix}
1 &amp; x_1\<br />
1 &amp; x_2\
\vdots &amp; \vdots \<br />
1 &amp; x_N
\end{pmatrix}, ~~~
\mathbf{w} =  \begin{pmatrix}
w_0\<br />
w_1
\end{pmatrix}
\end{equation}</p>
<details>
<summary>Simplification of the cost in more detail</summary>
<div>
\begin{align}
    C_2 &= \sum^N_{n = 1} \big[y_n - (w_1x_n + w_0)\big]^2\\
    ~\\
    &= \sum^N_{n = 1} \big[\mathbf{y}_n - \sum^2_{j = 1}\mathbf{X}_{nj}\mathbf{w}_j\big]^2\\
    ~\\
    &= \sum^N_{n = 1} \big[\mathbf{y}_n - \left(\mathbf{X}\mathbf{w}\right)_n\big]^2\\
    ~\\
    &= \big(\mathbf{y} - \mathbf{X}\mathbf{w}\big)^\top \big(\mathbf{y} - \mathbf{X}\mathbf{w}\big)\\
\end{align}
</div>
</details>
<p><strong>Step 2</strong>: using differentiation to solve for the minimum.</p>
<p>Letting the derivative of a quantity \(f\) with respect to a vector \(\mathbf{v}\) to have have elements</p>
<p>\[
\bigg(\frac{\partial f}{\partial \mathbf{v}}\bigg)_i = \frac{\partial f}{\partial \mathbf{v}_i}
\]</p>
<p>we differentiate \(C_2\), set to zero, and obtain the closed form solution:</p>
<p>\begin{align}\frac{\partial C_2}{\partial \mathbf{w}} &amp;= -2\mathbf{X}^\top\big(\mathbf{y} - \mathbf{X}\mathbf{w}\big)=0\
~\<br />
\implies &amp; \mathbf{X}^\top\mathbf{X}\mathbf{w} - \mathbf{X}^\top\mathbf{y} = 0\
~\<br />
\implies &amp;\boxed{\mathbf{w} = \big( \mathbf{X}^\top\mathbf{X}\big)^{-1}\mathbf{X}^\top \mathbf{y}}
\end{align}</p>
<details>
<summary>Derivatives in detail</summary>
<div>
Here we show a more detailed derivation of the equality $\frac{\partial C_2}{\partial \mathbf{w}} = -2\mathbf{X}^\top\big(\mathbf{y} - \mathbf{X}\mathbf{w}\big)$.
<p>\begin{align}
\bigg(\frac{\partial C_2}{\partial \mathbf{w}}\bigg)_i &amp;= \frac{\partial C_2}{\partial \mathbf{w}_i} = \frac{\partial}{\partial \mathbf{w}_i} \bigg[\big(\mathbf{y} - \mathbf{X}\mathbf{w}\big)^\top \big(\mathbf{y} - \mathbf{X}\mathbf{w}\big)\bigg] = \frac{\partial}{\partial \mathbf{w}_i} \sum_n \bigg[\big(\mathbf{y}<em>n - \sum_j\mathbf{X}</em>{nj}\mathbf{w}_j\big) \big(\mathbf{y}<em>n - \sum_j\mathbf{X}</em>{nj}\mathbf{w}_j\big)\bigg]\
~\
&amp;= 2\sum_n \bigg[\big(\mathbf{y}<em>n - \sum_j\mathbf{X}</em>{nj}\mathbf{w}_j\big) \frac{\partial}{\partial \mathbf{w}_i} \big(\mathbf{y}<em>n - \sum_j\mathbf{X}</em>{nj}\mathbf{w}_j\big)\bigg]\
~\
&amp;= -2\sum_n \bigg[\big(\mathbf{y}<em>n - \sum_j\mathbf{X}</em>{nj}\mathbf{w}<em>j\big) \big(\sum_j\mathbf{X}</em>{nj} \frac{\partial \mathbf{w}<em>j}{\partial \mathbf{w}<em>i}\big)\bigg]\
~\
&amp;= -2\sum_n \bigg[\big(\mathbf{y}<em>n - \sum_j\mathbf{X}</em>{nj}\mathbf{w}<em>j\big) \big(\sum_j\mathbf{X}</em>{nj} \delta</em>{ij}\big)\bigg]\
~\
&amp;= -2\sum_n \bigg[\big(\mathbf{y}<em>n - \sum_j\mathbf{X}</em>{nj}\mathbf{w}<em>j\big)\mathbf{X}</em>{ni}\bigg]\
~\
&amp;= -2\sum_n \bigg[\mathbf{X}^\top</em>{in}\big(\mathbf{y}<em>n - \sum_j\mathbf{X}</em>{nj}\mathbf{w}_j\big)\bigg]\
~\
&amp;= -2 \left[\mathbf{X}^\top \big(\mathbf{y} - \mathbf{X}\mathbf{w}\big)\right]_i\
\end{align}</p>
</div>
</details>
<p>As promised, when the matrix <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is invertable, we have a closed form solution for \(\mathbf{w}\) which extremizes \(C_2\). You can convince yourself this is a minimum either by taking a second derivative or by considering the quadratic form of \(C_2\) and what happens in the limit of large \(\mathbf{w}\).</p>
<div class="further_box">
** Further reading and information **<br><br>
<p>In the expression for the least squares solution for the weights</p>
<div class="math notranslate nohighlight">
\[\mathbf{w} = \big( \mathbf{X}^\top\mathbf{X}\big)^{-1}\mathbf{X}^\top \mathbf{y}\]</div>
<p>the matrix <span class="math notranslate nohighlight">\(\big( \mathbf{X}^\top\mathbf{X}\big)^{-1}\mathbf{X}^\top\)</span> is a generalization of the inverse of a matrix for non-square matrices, called the <strong><a class="reference external" href="http://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html">Moore-Penrose pseudoinverse</a></strong> having the property</p>
<div class="math notranslate nohighlight">
\[\bigg[\big( \mathbf{X}^\top\mathbf{X}\big)^{-1}\mathbf{X}^\top\bigg] \mathbf{X} = \big( \mathbf{X}^\top\mathbf{X}\big)^{-1}\mathbf{X}^\top\mathbf{X} = \mathbf{I}\]</div>
<p>Note that when <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is not invertable there is an infinity of solutions to the least squares problem. This occurs when the number of unique datapoints <span class="math notranslate nohighlight">\(N'\)</span> is smaller than the number of parameters we are fitting. For example, if we have a single datapoint, there are many ways of constructing a straight line that passes through the point. <br><br></p>
<p>More generally, when the inputs are <span class="math notranslate nohighlight">\(D\)</span> dimensional or when we’re fitting non-linear models with <span class="math notranslate nohighlight">\(D\)</span> basis functions the least squares solution is not unique when <span class="math notranslate nohighlight">\(N'&lt;D\)</span>. This is one of the motivations for approaches considered later that <a class="reference internal" href="regression_regularisation.html"><span class="doc std std-doc">regularise the least squares solution</span></a> and return unique solutions even when <span class="math notranslate nohighlight">\(N'&lt;D\)</span>.</p>
</div>
<p>Implementing this solution is straightforward because of the closed-form solution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x_lin</span><span class="p">)</span> <span class="c1"># create a vector of 1&#39;s with the same length as x</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">ones</span><span class="p">,</span> <span class="n">x_lin</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># stack 1&#39;s and x&#39;s to get the X matrix having the 1&#39;s and x&#39;s as columns</span>


<span class="c1"># compute the optimal w using the Moore-Penrose pseudoinverse</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y_lin</span><span class="p">)</span> <span class="p">)</span>

<span class="c1"># The above line is equivalent to the following: </span>
<span class="c1"># w = np.linalg.inv((X.T).dot(X)).dot(X.T).dot(y_lin) </span>
<span class="c1"># but unlike this approach it avoids explicitly computing the matrix inverse, which can be numerically unstable, </span>
<span class="c1"># instead it uses a linear solver, which is more stable. In general, if you don&#39;t explicitly need a matrix inverse, </span>
<span class="c1"># you should avoid computing it.</span>

<span class="n">x_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="c1"># 100 points equispaced between 0 and 1</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_pred</span><span class="p">)</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># evaluate the linear trendline at the values of x above</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span> <span class="c1"># plot the trendline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_lin</span><span class="p">,</span> <span class="n">y_lin</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">)</span> <span class="c1"># plot the datapoints</span>
<span class="n">beautify_plot</span><span class="p">({</span><span class="s2">&quot;title&quot;</span><span class="p">:</span><span class="s2">&quot;Least squares regression&quot;</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">:</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="s2">&quot;$y$&quot;</span><span class="p">})</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">toggle_code</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">17</span><span class="o">-</span><span class="n">eb8fbec44e06</span><span class="o">&gt;</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_pred</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;black&#39;</span><span class="p">)</span> <span class="c1"># plot the trendline</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span> <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_lin</span><span class="p">,</span> <span class="n">y_lin</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span><span class="p">)</span> <span class="c1"># plot the datapoints</span>
<span class="ne">---&gt; </span><span class="mi">21</span> <span class="n">beautify_plot</span><span class="p">({</span><span class="s2">&quot;title&quot;</span><span class="p">:</span><span class="s2">&quot;Least squares regression&quot;</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">:</span><span class="s2">&quot;$x$&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span><span class="s2">&quot;$y$&quot;</span><span class="p">})</span>
<span class="g g-Whitespace">     </span><span class="mi">22</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">23</span> 

<span class="ne">NameError</span>: name &#39;beautify_plot&#39; is not defined
</pre></div>
</div>
<img alt="../../_images/regression-linear_4_1.png" src="../../_images/regression-linear_4_1.png" />
</div>
</div>
<p><strong>Critique of the Least Squares fitting approach</strong></p>
<p>Is the least squares procedure a sensible way to fit a linear model and return predictions?</p>
<p>One weakness in the rationale for the approach outlined above is that the cost function was drawn out of a hat. For example, instead of minimising the squared error, why not use an alternative cost in the family</p>
<p>\begin{align}
C_{p} = \sum^N_{n = 1} \big|y_n - (w_1x_n + w_0)\big|^p,~\text{for some <span class="math notranslate nohighlight">\(p &gt; 0\)</span>}.
\end{align}</p>
<p>From a <em>computational</em> perspective, minimising \(C_2\) is arguably a sensible choice as it yields a closed-form solution for the parameters (other choices do not), but it is not immediately clear if it is a <em>statistically</em> superior approach. However, in the next section we will show that minimising \(C_2\) is equivalent to a probabilistic approach that finds the parameters that are most likely, given the data, under the assumption of Gaussian observation noise.</p>
<p>Another weakness in the least squares approach is that it does not return uncertainty estimates in either the parameter estimates or the predictions. Framing the estimation as a probabilistic inference problem will later enable us to apply full blown probabilistic inference which does return uncertainties.</p>
</div>
<div class="section" id="maximum-likelihood-fitting">
<h2>Maximum likelihood fitting<a class="headerlink" href="#maximum-likelihood-fitting" title="Permalink to this headline">¶</a></h2>
<p>The least squares approach assumed that the function underlying the data was a straight line, but it did not explicitly specify how the observed data relates to this line. One simple assumption is that they are produced by adding independent and identically distributed Gaussian noise with zero mean and variance <span class="math notranslate nohighlight">\(\sigma_{y}^2\)</span>,</p>
<p>\[y_n = w_0 + w_1 x_n + \epsilon_n,~\text{where}~ \epsilon_n \sim \mathcal{N}(0, \sigma_{y}^2).\]</p>
<p>This expression describes how to sample the outputs given the inputs and the model parameters, by computing the value the linear function takes at <span class="math notranslate nohighlight">\(x_n\)</span> and adding Gaussian noise. We can write down an equivalent expression for the probability density of the outputs <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> given the inputs and model parameters (<span class="math notranslate nohighlight">\(\mathbf{X}, \mathbf{w}, \sigma_y^2\)</span>),</p>
<p>\[p(\mathbf{y}\mid\mathbf{X}, \mathbf{w}, \sigma_y^2) = \frac{1}{(2\pi \sigma_y^2)^{N/2}}\text{exp}\big(-\frac{1}{2\sigma_y^2}(\mathbf{y} - \mathbf{X}\mathbf{w})^\top (\mathbf{y} - \mathbf{X}\mathbf{w})\big)\]</p>
<p>This function is also known as the <strong>likelihood of the parameters</strong> and it can be used for fitting. The <strong>setting of the parameters that makes the observed data most probable is called the  maximum likelihood estimate</strong>.</p>
<p>One way to find the parameters \(\mathbf{w}\) that maximise the likelihood is by directly taking derivatives of <span class="math notranslate nohighlight">\(p(\mathbf{y}\mid\mathbf{X}, \mathbf{w}, \sigma_y^2)\)</span>. However, we can make our lives easier by maximising the log of the likelihood. This is equivalent since the logarithm is a monotonically increasing function (that is <span class="math notranslate nohighlight">\(\log(x_1) &gt; \log(x_2)\)</span> if <span class="math notranslate nohighlight">\(x_1 &gt; x_2\)</span> ), and the location of maxima and minima are unchanged by applying a monotonically increasing function. <strong>It is often simpler to handle logarithms of probabilistic quantities</strong> since:</p>
<ol class="simple">
<li><p>probabilities often decompose into products which become sums once the logarithm is taken, and</p></li>
<li><p>many widely used probability distributions often involve the exponential function which simplifies after taking the log (the Gaussian being such a case).</p></li>
</ol>
<details>
<summary>More details on the extrema of monotonically transformed functions</summary>
<div>
Consider a quantity \\(\mathcal{Q}(x)\\) with a (local or global) maximum at \\(x^*\\), and a  monotonic function \\(f\\).<br><br> 
<p>Now consider <span class="math notranslate nohighlight">\(Q\)</span> in a neighbourhood around <span class="math notranslate nohighlight">\(x^*\)</span>.  Denoting a point in this neighbourhood as <span class="math notranslate nohighlight">\(x^*+\delta\)</span> we can restrict the size of neighbourhood by limiting the magnitude of the perturbation <span class="math notranslate nohighlight">\(|\delta| &lt; \epsilon\)</span>.  For sufficiently small <span class="math notranslate nohighlight">\(\epsilon\)</span> due to  <span class="math notranslate nohighlight">\(x^*\)</span> being a maximum, <span class="math notranslate nohighlight">\(Q(x^*)&gt;Q(x^*+\delta)\)</span>. <br><br></p>
<p>Now, we can apply the function <span class="math notranslate nohighlight">\(f\)</span> to both sides of this inequality and use the fact that it is monotonic to give</p>
<p>\begin{align}
f(Q(x^<em>))&gt;f(Q(x^</em>+\delta)) ;; \forall ;; |\delta| &lt; \epsilon
\end{align}</p>
<p>This demonstrates that <span class="math notranslate nohighlight">\(x^*\)</span> is also a (local/global) maximum of <span class="math notranslate nohighlight">\(f(Q(x))\)</span>.</p>
</div>
</details>
<p>The <strong>log-likelihood</strong> is</p>
<p>\[\mathcal{L}(\mathbf{w}) = \text{log}~ p(\mathbf{y}\mid\mathbf{X}, \mathbf{w}, \sigma_y^2) = -\frac{N}{2}\log(2\pi \sigma_y^2) -\frac{1}{2\sigma_y^2}(\mathbf{y} - \mathbf{X}\mathbf{w})^\top (\mathbf{y} - \mathbf{X}\mathbf{w})\]</p>
<p>maximising this quantity is equivalent to <strong>minimising</strong> the negative log-likelihood</p>
<p>\[-\mathcal{L}(\mathbf{w}) = \frac{N}{2}\log(2\pi \sigma_y^2) +\frac{1}{2\sigma_y^2}(\mathbf{y} - \mathbf{X}\mathbf{w})^\top (\mathbf{y} - \mathbf{X}\mathbf{w})\]</p>
<p>Notice that the term \(\frac{N}{2}\log(2\pi \sigma_y^2)\) is independent of \(\mathbf{w}\), so minimising the negative log-likelihood is equivalent to minimizing the least-squares error \(-\) exactly the same criterion we had before:</p>
<p>\[\boxed{\text{Least squares} \equiv \text{minimize}~ (\mathbf{y} - \mathbf{X}\mathbf{w})^\top (\mathbf{y} - \mathbf{X}\mathbf{w}) \Leftrightarrow \text{Maximum-likelihood}}\]</p>
<p>So the squared error assumption is equivalent to assuming that the observed data have been corrupted by Gaussian noise whose variance is fixed across the input space. One of the benefits of this new perpective (i.e. in terms of a probabilistic model and a specific inference scheme) is that the implicit assumptions are revealed, their suitability assessed, and  modifications can be made. For example, the model could be generalised to include noise whose variance depends on the input location. Moreover, it might be important to infer the noise level \(\sigma_y\). The maximum likelihood approach paves the way to the Bayesian modelling approach where the task of infering the model weights retains uncertainty and incorporates prior knowledge.</p>
<p>In the next <a class="reference internal" href="regression_non_linear.html"><span class="doc std std-doc">notebook</span></a>, we look at how to adapt our linear regression model to better deal with non-linear datasets that cannot be so accurately fit by a straight line.</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>Having covered linear regression, you should now understand:</p>
<ol class="simple">
<li><p>Why the sum-of-squared-errors is used as a measure of fit</p></li>
<li><p>How to manipulate the sum-of-square-errors cost into different forms</p></li>
<li><p>How to derive the least squares estimate for the parameters of a linear model</p></li>
<li><p>Why the least squares estimate is equivalent to the maximum likelihood estimate (under an i.i.d. Gaussian noise assumption)</p></li>
</ol>
</div>
<div class="section" id="questions">
<h2>Questions<a class="headerlink" href="#questions" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p><strong>Probabilistic models for regression</strong></p></li>
</ol>
<p>A machine learner observes two separate regression datasets comprising scalar inputs and outputs <span class="math notranslate nohighlight">\(\{ x_n, y_n \}_{n=1}^N\)</span> shown below.</p>
  <div class="row">
  <div class="column">
    <img src="dataset_regression.svg" alt="Snow" style="width:90%; float: center; padding: 0px; padding : 20px">
  </div>
</div>
<ul class="simple">
<li><p>Suggest a suitable regression model, <span class="math notranslate nohighlight">\(p(y_n|x_n)\)</span> for the dataset A. Indicate sensible settings for the parameters in your proposed model where possible. Explain your modelling choices.</p></li>
<li><p>Suggest a suitable regression model, <span class="math notranslate nohighlight">\(p(y_n|x_n)\)</span> for the dataset B. Indicate sensible settings for the parameters in your proposed model where possible. Explain your modelling choices.</p></li>
</ul>
<!-- <details>
<summary>Answer</summary>
<div class="row">
  <div class="column">
    <img src="solution-prob_mod.png" alt="Snow" style="width:95%; float: center; padding: 0px; padding : 20px">
  </div>
</div>
</details> -->
<ol class="simple">
<li><p><strong>Maximum-likelihood learning for a simple regression model</strong></p></li>
</ol>
<p>Consider a regression problem where the data comprise <span class="math notranslate nohighlight">\(N\)</span> scalar inputs and outputs, <span class="math notranslate nohighlight">\(\mathcal{D} = \{ (x_1, y_1), ..., (x_N,y_N)\}\)</span>, and the goal is to predict <span class="math notranslate nohighlight">\(y\)</span> from <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>Assume a very simple linear model, <span class="math notranslate nohighlight">\(y_n = a x_n + \epsilon_n\)</span>, where the noise <span class="math notranslate nohighlight">\(\epsilon_n\)</span> is iid Gaussian with zero mean and variance 1.</p>
<ul class="simple">
<li><p>Provide an expression for the log-likelihood of the parameter <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p>Compute the maximum likelihood estimate for <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
</ul>
<details>
<summary>Answer</summary>
<div class="row">
  <div class="column">
    <img src="solution-ml.png" alt="Snow" style="width:95%; float: center; padding: 0px; padding : 20px">
  </div>
</div>
</details>
<ol class="simple">
<li><p><strong>Maximum-likelihood learning for multi-output regression</strong></p></li>
</ol>
<p>A data-scientist has collected a regression dataset comprising <span class="math notranslate nohighlight">\(N\)</span> scalar inputs (<span class="math notranslate nohighlight">\(\{x_n\}_{n=1}^N\)</span>) and <span class="math notranslate nohighlight">\(N\)</span> scalar outputs (<span class="math notranslate nohighlight">\(\{y_n\}_{n=1}^N\)</span>). Their goal is to predict <span class="math notranslate nohighlight">\(y\)</span> from <span class="math notranslate nohighlight">\(x\)</span> and they have assumed a very simple linear model, <span class="math notranslate nohighlight">\(y_n = a x_n + \epsilon_n\)</span>.</p>
<p>The data-scientist also has access to a second set of outputs (<span class="math notranslate nohighlight">\(\{z_n\}_{n=1}^N\)</span>) that are well described by the model <span class="math notranslate nohighlight">\(z_n = x_n + \epsilon'_n\)</span>.</p>
<p>The noise variables <span class="math notranslate nohighlight">\(\epsilon_n\)</span> and <span class="math notranslate nohighlight">\(\epsilon'_n\)</span>  are known to be iid zero mean correlated Gaussian variables</p>
<p>\begin{align}
p\left( \left[ \begin{array}{c} \epsilon_n\ \epsilon’_n \end{array} \right ] \right) = \mathcal{N}\left( \left[ \begin{array}{c} \epsilon_n\ \epsilon’_n \end{array} \right ]; \mathbf{0}, \Sigma \right)  ;; \text{where} ;; ; \Sigma^{-1} = \left[ \begin{array}{cc} 1 &amp; 0.5 \ 0.5 &amp; 1 \end{array} \right ].  \nonumber
\end{align}</p>
<ul class="simple">
<li><p>Provide an expression for the log-likelihood of the parameter <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p>Compute the maximum likelihood estimate for <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p>Do the additional outputs <span class="math notranslate nohighlight">\(\{z_n\}_{n=1}^N\)</span> provide useful additional information for estimating <span class="math notranslate nohighlight">\(a\)</span>? Explain your reasoning.</p></li>
</ul>
<p>The formula for the probability density of a multivariate Gaussian distribution of mean <span class="math notranslate nohighlight">\(\mu\)</span> and covariance <span class="math notranslate nohighlight">\(\Sigma\)</span> is given by
\begin{align}
\mathcal{N}(\mathbf{x};\mu,\Sigma) = \frac{1}{\sqrt{\text{det}(2 \pi \Sigma)}} \exp\left(-\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^{\top} \Sigma^{-1}  (\mathbf{x} - \mathbf{\mu})\right). \nonumber
\end{align}</p>
 <details>
<summary>Answer</summary>
<div class="row">
  <div class="column">
    <img src="solution-multi-output-1.png" alt="Snow" style="width:95%; float: center; padding: 0px; padding : 20px">
          <img src="solution-multi-output-2.png" alt="Snow" style="width:95%; float: center; padding: 0px; padding : 20px">
  </div>
</div>
</details></div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="regression-intro.html" title="previous page">Regression</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Stratis Markou, Rich Turner<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../../_static/js/index.3da636dd464baa7582d2.js"></script>


    
    <!-- Google Analytics -->
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-168728006-1', 'auto');
      ga('set', 'anonymizeIp', true);
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
    <!-- End Google Analytics -->
    
  </body>
</html>