{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helper_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-77aa9334fdd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# set things like fonts etc - comes from helper_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helper_functions'"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'svg' # change output plot display format to 'svg'\n",
    "\n",
    "# import the required modules for this notebook\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import the helper functions from the parent directory,\n",
    "# these help with things like graph plotting and notebook layout\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from helper_functions import *\n",
    "\n",
    "# set things like fonts etc - comes from helper_functions\n",
    "set_notebook_preferences()\n",
    "\n",
    "# add a show/hide code button - also from helper_functions\n",
    "toggle_code(title = \"setup code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class softmax classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now extend the [binary logistic regression model](classification_logistic_regression_ML_fitting.ipynb) to handle multiple classes. \n",
    "\n",
    "###  The Softmax Classification Model\n",
    "\n",
    "As before, each datapoint comprises an input $\\mathbf{x}_n$ and an output value $y_n$. Now the output $y_n = k$ indicates which of $K$ classes the $n^{th}$ datapoint belongs to. Binary classification is recovered when $K=2$.\n",
    "\n",
    "The softmax model also comprises two stages. The first stage computes $k$ activations $a_{n,k} = \\mathbf{w}_k^\\top \\mathbf{x}_n$ using a weight vector for each class $\\{\\mathbf{w}_k\\}_{k=1}^K$. The second stage passes this set of activations into a **softmax function** which returns the probability that the datapoint belongs to each of the K classes (i.e. a K dimensional vector of probabilities) which has elements \n",
    "\n",
    "$$\n",
    "p(y_{n} = k |\\mathbf{x}_n, \\{\\mathbf{w}_k\\}_{k=1}^K) = \\frac{\\exp(a_{n,k})}{\\sum_{k'=1}^K \\text{exp}(a_{n,k'})} = \\frac{\\text{exp}(\\mathbf{w}_k^\\top \\mathbf{x}_n)}{\\sum_{k'=1}^K \\exp(\\mathbf{w}_{k'}^\\top \\mathbf{x}_n)},\n",
    "$$\n",
    "\n",
    "Notice that by construction the softmax function is normalised \n",
    "\n",
    "$$\\sum_{k=1}^K p(y_{n} = k |\\mathbf{x}_n, \\{\\mathbf{w}_k\\}_{k=1}^K) = \\frac{\\sum_{k=1}^K \\exp(a_{n,k})}{\\sum_{k'=1}^K \\text{exp}(a_{n,k'})} = 1$$\n",
    "\n",
    "In this way, the softmax parameterises a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution) over the output variable, whereas the logistic function parameterised a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution).\n",
    "\n",
    "Let's plot some examples of this model for K=3 classes and D=2 dimensional inputs. Each column of the plot below corresponds to a different softmax model each with its own weights $W^{(m)} = [\\mathbf{w}_1^{(m)},\\mathbf{w}_2^{(m)},\\mathbf{w}^{(m)}_3]$. Each row shows how the probability of one of the three classes varies across the input space. The probability contours are not linear, however it turns out that the decision boundaries are. For more insight into the softmax function, see [question 1](#Questions) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ebd08698827f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;31m# define softmax function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mno_increments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x), axis = 1)).T # define softmax function\n",
    "\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "\n",
    "no_increments = 100\n",
    "\n",
    "x1 = np.linspace(-5,5,no_increments)\n",
    "x2 = np.linspace(-5,5,no_increments)\n",
    "\n",
    "for w in range(0,3):\n",
    "    \n",
    "    W = np.random.randn(3,3)\n",
    "    \n",
    "    grid = np.stack(np.meshgrid(x1,x2), axis = -1)\n",
    "\n",
    "    grid = grid.reshape((-1, 2))\n",
    "\n",
    "    grid = np.append(np.ones(shape = (grid.shape[0], 1)), grid, axis = 1)\n",
    "    \n",
    "    probs = softmax(grid.dot(W)).reshape((no_increments, no_increments, 3))\n",
    "    \n",
    "    for class_ in range(1,4):\n",
    "        \n",
    "        # adding a subplot in appropriate location\n",
    "        ax = fig.add_subplot(3, 3, (class_-1) * 3 + (w+1))\n",
    "        \n",
    "        ax.contourf(x1,x2,probs[:,:,class_-1],cmap=matplotlib.cm.coolwarm,alpha=0.5)\n",
    "        \n",
    "        if w == 0:\n",
    "            \n",
    "            beautify_plot({\"y\":\"Class \" + str(class_)})\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            plt.yticks([])\n",
    "            \n",
    "        if class_ == 1:\n",
    "            \n",
    "            beautify_plot({\"title\":r\"$W^{(\" + str(w) + \")}$\"})\n",
    "            \n",
    "        if class_ != 3:\n",
    "        \n",
    "            plt.xticks([])\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.4,wspace = 0.3)\n",
    "plt.show()\n",
    "        \n",
    "toggle_code()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting using maximum-likelihood estimation\n",
    "\n",
    "Having gained some intuition about the model, we now consider maximum likelihood fitting. \n",
    "\n",
    "The likelihood can be written in a compact form using a one-hot encoding of the class labels. That is, we can encode the output $y_n=k$ into a vector of length $K$ comprising $K-1$ zeros and a one in the $k^{\\text{th}}$ element e.g. for $K=4$ classes if the $n^{\\text{th}}$ datapoint belongs to the third class $y_n=3$ we have $\\mathbf{y}_n = [0,0,1,0]$. These one-hot encodings can be stacked into an N-by-K matrix with elements $y_{n,k}$ and will help us do the book keeping associated with computing the correct output probability for each datapoint. Armed with this new representation of the output, we can write the probability of the output given the weights and the inputs as,\n",
    "\n",
    "\\begin{align}\n",
    "p(\\{y_{n}\\}_{n=1}^N|\\{\\mathbf{x}_n\\}_{n=1}^N, \\{\\mathbf{w}_k\\}_{k=1}^K) &= \\prod_{n = 1}^N \\prod_{k = 1}^K s_{n,k}^{y_{n,k}}.\n",
    "\\end{align}\n",
    "\n",
    "Here we have denoted the output of the softmax function for each datapoint as $s_{n,k} = p(y_{n} = k |\\mathbf{x}_n, \\{\\mathbf{w}_k\\}_{k=1}^K) = \\text{exp}(\\mathbf{w}_k^\\top \\mathbf{x}_n)\\big/\\sum_{k'} \\exp(\\mathbf{w}_{k'}^\\top \\mathbf{x}_n)$. \n",
    "\n",
    "In case it is not clear, let's briefly consider the the trick of raising the softmax output to the power $y_{n,k}$ through a simple example. Let $N=1$ and $y_1 = 3$ so that $\\mathbf{y}_1 = [y_{1,1},y_{1,2},y_{1,3},y_{1,4}] = [0,0,1,0]$. The right hand side of the likelihood is therefore\n",
    "\n",
    "$$\\prod_{k = 1}^K s_{n,k}^{y_{n,k}} = s_{n,1}^{0} s_{n,2}^{0} s_{n,3}^{1} s_{n,4}^{0} = s_{n,3}^{1} = p(y_{1} = 3 |\\mathbf{x}_n, \\{\\mathbf{w}_k\\}_{k=1}^K).$$\n",
    "\n",
    "So everything works as it should do. The probability of the output given the weights and the inputs is also called the likelihood of the parameters. The log-likelihood is therefore\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\{\\mathbf{w}\\}_{k=1}^K) &= \\sum_{n = 1}^N \\sum_{k = 1}^K y_{n,k} \\log s_{n,k}\n",
    "\\end{align}\n",
    "\n",
    "Now we can numerically optimise the log-likelihood using gradient ascent. This requires computation of the derivatives of $\\mathcal{L}(\\{\\mathbf{w}\\}_{k=1}^K)$ with respect to the weights, \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}(\\{\\mathbf{w}\\}_{k=1}^K)}{\\partial \\mathbf{w}_j} = \\sum^N_{n = 1} (y_{n,j} - s_{n,j}) \\mathbf{x}_n.\n",
    "\\end{align}\n",
    "\n",
    "The full derivation of the gradient can be found below. Notice how it is composed of a sum over contributions from each datapoint, and each contribution involves the prediction error $y_{n,j} - s_{n,j}$ multiplied by the input $\\mathbf{x}_n$. \n",
    "\n",
    "<details>\n",
    "<summary>Derivation of the gradient of the softmax log-likelihood</summary>\n",
    "<div>\n",
    "    Starting from the expression\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\{\\mathbf{w}\\}_{k=1}^K) &= \\sum_{n = 1}^N \\sum_{k = 1}^K y_{n,k} \\text{log}~s_{n,k},\n",
    "\\end{align}\n",
    "\n",
    "and taking the derivative w.r.t. $\\mathbf{w}_j$ we see:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}(\\{\\mathbf{w}\\}_{k=1}^K)}{\\partial \\mathbf{w}_j} &= \\sum_{n = 1}^N\\sum_{k = 1}^K y_{n,k} \\frac{1}{s_{n,k}} \\frac{\\partial s_{n,k}}{\\partial \\mathbf{w}_j} = \\sum_{n = 1}^N\\sum_{k = 1}^K y_{n,k} \\frac{1}{s_{n,k}} \\frac{\\partial s_{n,k}}{\\partial a_{n,j}} \\frac{\\partial a_{n,j}}{\\mathbf{w}_j}\\\\\n",
    "~\\\\\n",
    "&= \\sum_{n = 1}^N\\sum_{k = 1}^K y_{n,k} (\\delta_{k,j} - s_{n,j}) \\mathbf{x}_n\\\\\n",
    "~\\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\delta_{k,j}$ is a [Kronecker delta](https://en.wikipedia.org/wiki/Kronecker_delta) and we have used the identity\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial s_{n,k}}{ \\partial a_{n,j}} = s_{n,k}(\\delta_{k,j} - s_{n,j}).\n",
    "\\end{align}\n",
    "\n",
    "Then considering that for each $n$, $y_{n,k}$ is $1$ for a single value of $k$ and $0$ for all other values of $k$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L}(\\{\\mathbf{w}\\}_{k=1}^K)}{\\partial \\mathbf{w}_j} &= \\sum_{n = 1}^N\\sum_{k = 1}^K y_{n,k}(\\delta_{k,j} - s_{nj})\\mathbf{x}_n\\\\\n",
    "~\\\\\n",
    "&= \\sum_{n = 1}^N\\sum_{k = 1}^K (y_{n,j} - s_{n,j})\\mathbf{x}_n\\\\\n",
    "\\end{align}\n",
    "\n",
    "arriving at the final result.\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write down code for gradient ascent of the softmax classification likelihood, just as we did for logistic classication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toggle_code' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6c8eadfb00cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_liks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtoggle_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient ascent algorithm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_load_hide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'toggle_code' is not defined"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x), axis = 1)).T # define softmax function for convenience\n",
    "\n",
    "def softmax_gradient_ascent(x, y, init_weights, no_steps, stepsize):\n",
    "    \n",
    "    x = np.append(np.ones(shape = (x.shape[0], 1)), x, axis = 1) # add 1's to the inputs as before\n",
    "    \n",
    "    w = init_weights.copy() # copy weights as before\n",
    "    \n",
    "    w_history, log_liks = [], [] # arrays for storing weights and log-liklihoods as before\n",
    "\n",
    "    for n in range(no_steps): # in this part we optimise log-lik w.r.t. ws\n",
    "        \n",
    "        log_liks.append(np.sum(y*np.log(softmax(x.dot(w))))) # record current log-lik as before\n",
    "        \n",
    "        w_history.append(w.copy()) # record current weights as before\n",
    "    \n",
    "        soft_ = softmax(x.dot(w)) # using our neat convenience function\n",
    "        \n",
    "        dL_dw = (x.T).dot(y - soft_)/x.shape[0]\n",
    "        \n",
    "        w += stepsize*dL_dw # update weights and repeat\n",
    "    \n",
    "    return np.array(w_history), np.array(log_liks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run gradient ascent on the Iris dataset with all 3 classes included. Let's start by running it with just two of the input dimensions retained (sepal length and width) so that we can visualise the results easily. In the plot below, each point is coloured with the RGB value representing the probabilistic prediction the trained model makes for each class ($0$,$1$ and $2$ respectively) at this location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-172e01a4e3e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iris_inputs_full.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iris_labels.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.load('iris_inputs_full.npy')\n",
    "y = np.load('iris_labels.npy')\n",
    "\n",
    "y_ = np.zeros(shape = (y.shape[0], 3))\n",
    "y_[np.arange(y.shape[0]), y] = 1\n",
    "\n",
    "no_train = (x.shape[0] * 3) // 4\n",
    "x_train, x_test, y_train, y_test = x[:no_train, :2], x[no_train:, :2], y_[:no_train], y_[no_train:]\n",
    "\n",
    "init_weights = np.zeros(shape = (x_train.shape[1] + 1, 3))\n",
    "\n",
    "w_history, log_liks = softmax_gradient_ascent(x_train, y_train, init_weights, 10 ** 4, 0.05)\n",
    "\n",
    "min_x, max_x, min_y, max_y, res = 3, 9, 0, 6, 1000\n",
    "grid = np.stack(np.meshgrid(np.linspace(min_x, max_x, res), np.linspace(min_y, max_y, res)), axis = -1)\n",
    "grid = grid.reshape((-1, 2))\n",
    "grid = np.append(np.ones(shape = (grid.shape[0], 1)), grid, axis = 1)\n",
    "\n",
    "probs = softmax(grid.dot(w_history[-1])).reshape((res, res, 3))\n",
    "rgb_colors = np.roll(probs, -1, axis  = -1)\n",
    "plt.imshow(rgb_colors, extent = [min_x, max_x, min_y, max_y], alpha = 0.5, origin='lower')\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], marker = 'x', s = 20, color = np.array(['b', 'r', 'g'])[y[:no_train]])\n",
    "beautify_plot({\"title\":r\"Likelihood in data space\", \"x\":\"Sepal length (cm)\", \"y\":\"Sepal width (cm)\"})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have visualised the probabilities, we can apply the gradient-ascent algorithm to the full Iris dataset, with all of the input dimesions retained. First lets see how the log-likelihood changes with iteration number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-be5f7890707a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iris_inputs_full.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iris_labels.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.load('iris_inputs_full.npy')\n",
    "y = np.load('iris_labels.npy')\n",
    "\n",
    "y_ = np.zeros(shape = (y.shape[0], 3))\n",
    "y_[np.arange(y.shape[0]), y] = 1\n",
    "\n",
    "no_train = (x.shape[0] * 3) // 4\n",
    "x_train, x_test, y_train, y_test = x[:no_train], x[no_train:], y_[:no_train], y_[no_train:]\n",
    "\n",
    "init_weights = np.zeros(shape = (x_train.shape[1] + 1, 3))\n",
    "\n",
    "w_history, log_liks = softmax_gradient_ascent(x_train, y_train, init_weights, 10 ** 4, 0.05)\n",
    "beautify_plot({\"title\":r\"Optimisation of $\\mathcal{L}$\", \"x\":\"Step #\", \"y\":\"Log-likelihood $\\mathcal{L}$\"})\n",
    "plt.plot(log_liks, color = 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test the accuracy of our model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-06188231ec00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Classification accuracy for full iris dataset = {0:.1f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtoggle_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "def softmax_test_accuracy(test_x, test_y, w):\n",
    "    x_ = np.append(np.ones(shape = (test_x.shape[0], 1)), test_x, axis = 1)\n",
    "    y_ = softmax(x_.dot(w))\n",
    "    return (np.argmax(y_, axis = 1) == np.argmax(y_test, axis = 1)).mean()\n",
    "\n",
    "print('Classification accuracy for full iris dataset = {0:.1f}%'.format(softmax_test_accuracy(x_test, y_test, w_history[-1])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The multi-class softmax classification model comprises two steps:  \n",
    "\n",
    "1. Compute $K$ activations, one for each class, each of which are linear projections of the input $a_{n,k} = \\mathbf{w}_k^\\top \\mathbf{x}_n$\n",
    "2. Pass the $K$ activations into the softmax function to get a vector of $K$ elements which are the class membership probabilities $p(y_{n} = k |\\mathbf{x}_n, \\{\\mathbf{w}_k\\}_{k=1}^K) = \\text{exp}(\\mathbf{w}_k^\\top \\mathbf{x}_n)\\big/\\sum_{k'} \\exp(\\mathbf{w}_{k'}^\\top \\mathbf{x}_n)$\n",
    "\n",
    "The log-likelihood and its derivatives can be compactly written using a one-hot encoding of the training data's outputs. Gradient ascent can then be used to numerically optimise the log-likelihood. \n",
    "\n",
    "In the [next section](classification_non-linear.ipynb) we will look at how to generalise this method to non-linear classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. **Why the name 'softmax' and relating softmax classification to logistic classification**\n",
    "\n",
    "Consider the softmax classification function:\n",
    "\n",
    "$$\n",
    "p(y_{k} = k |\\mathbf{x}, \\{\\mathbf{w}_k\\}_{k=1}^K)  = \\frac{\\text{exp}(\\mathbf{w}_k^\\top \\mathbf{x})}{\\sum_{k'=1}^K \\exp(\\mathbf{w}_{k'}^\\top \\mathbf{x})},\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "What happens to the softmax function as the magnitude of the weights tends to infinity $|\\mathbf{w}_k| \\rightarrow \\infty$?\n",
    "\n",
    "Consider K=2 classes, compare and contrast the softmax classification model to binary logistic classification. Is the softmax function **[identifiable](https://en.wikipedia.org/wiki/Identifiability)**?\n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "<div>\n",
    "   \n",
    "   <div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img src=\"softmax-solution-1.png\" alt=\"Snow\" style=\"width:100%; float: center; padding: 0px\">\n",
    "    <img src=\"softmax-solution-2.png\" alt=\"Snow\" style=\"width:100%; float: center; padding: 0px\">\n",
    "  </div>\n",
    "    </div>\n",
    "    Notice that although the softmax and logistic functions are identical for $K=2$ classes, the parameterisation is different. The softmax version is over-parameterised having two sets of parameters whose difference affects the input-output function. For this reason the parameters of the softmax are not identifiable: adding the same vector to each weight $\\mathbf{w}_k \\leftarrow \\mathbf{w}_k + \\mathbf{b}$ causes no change in the input-output function.\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "2. **Making multi-class classifiers from binary classifiers**\n",
    "\n",
    "Alice has a multi-class classification problem, but only has access to code for training and making predictions from a binary classifier. Devise heuristic approaches for using a set of binary classiers to solve a multi-class problem. Compare and contrast these approaches to softmax classification. \n",
    "\n",
    "<details>\n",
    "<summary>Answer</summary>\n",
    "<div>\n",
    "There are a variety of ways of transforming a multi-class classification problem to a binary one (see [here](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest))    <br><br>\n",
    "    \n",
    "A simple technique is to train K all-versus-one classifiers each of which classifies an input into either belonging to class k or one of the other K-1 classes. A test point can be run through each of these classifiers and the largest output picked. <br><br>\n",
    "\n",
    "Alternatively, a set of pairwise binary classifiers could be built, potentially for all K(K-1)/2 pairs of classification problems and the winning class selected by majority vote.<br><br>\n",
    "\n",
    "Another approach is to build a tree of classifiers. E.g. if there are four classes, the root classifier might first split the input into classes (1 & 2) vs (3 & 4) with two leaf classifiers making the final classification (1 vs 2) and (3 vs 4). This approach would involve building the tree (i.e. figuring out which classes to group together at the non-leaf nodes). **Decision trees** and **random forests** take approaches of this sort. \n",
    "</div>\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
