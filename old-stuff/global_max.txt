This is a sketch of a proof that the likelihood of a probabilistic linear classification model, given by

\begin{align}
p(\mathbf{y}|\{\mathbf{x}_n\} \mathbf{w}) = \prod^N_{n = 1} p(y_n|\mathbf{x}_n, \mathbf{w}),~~~p(y_n|\mathbf{x}_n, \mathbf{w}) = \sigma(\mathbf{w}^\top\mathbf{x}_n)
\end{align}

where $\sigma$ is a strictly monotonic function (i.e. always increases or decreases) has a single (global) maximum. We sketch a proof of this result using induction, by showing that for a function $f_{M}$ depending on the weights $\mathbf{w}$ and first $M$ datapoints, $\{\mathbf{x}_m\}_{m = 1}^M$, written $f_{M}(\mathbf{w}, \{\mathbf{x}\}_{m=1}^{M})$ with a single maximum (which may be at infinity), the function $f_{M+1}$ given by

\begin{align}
f_{M+1}(\mathbf{w}, \{\mathbf{x}\}_{m=1}^{M+1}) = f_{M}(\mathbf{w}, \{\mathbf{x}\}_{m = 1}^{M}) \sigma(\mathbf{w}^\top\mathbf{x}_{M+1})
\end{align}

also has a single maximum. Since $\sigma$ has a single maximum (at infinity), it

