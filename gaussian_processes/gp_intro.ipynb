{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes\n",
    "\n",
    "In this chapter, we will explore Gaussian Processes (GPs), motivated by the problem of regression. Gaussian Processes provide a natural way to model data in a bayesian way, while resolving issues we encountered in [linear regression](../regression_intro.ipynb), such as choosing the number or type of basis functions.\n",
    "\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img src=\"intro-regression.svg\" alt=\"no image :(\" style=\"width:80%; float: center; padding: 0px; padding : 20px\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "As a reminder, given a set of datapoints $\\{\\mathbf{x}_n, \\mathbf{y}_n\\}_{n = 1}^N$, regression amounts to fitting a function $\\mathbf{f}(\\mathbf{x}_n)$ to $\\mathbf{y}_n$, as illustrated by the figure. Previously, we considered linear-in-the-parameters models:\n",
    "\n",
    "$$\\mathbf{y}_n = w_1\\boldsymbol{\\phi}_1(\\mathbf{x}_n) + w_2\\boldsymbol{\\phi}_2(\\mathbf{x}_n) + ... + w_d\\boldsymbol{\\phi}_d(\\mathbf{x}_n) + \\boldsymbol{\\epsilon}_n = \\boldsymbol{\\Phi}\\mathbf{w} + \\boldsymbol{\\epsilon}_n.$$\n",
    "\n",
    "which we trained by maximum-likelihood. We then added regularisation to prevent overfitting and showed that L2 regularisation is equivalent to placing a gaussian prior, $p(\\mathbf{w})$, over the weights. Using Bayes' rule, we obtained the posterior distribution\n",
    "\n",
    "$$p(\\mathbf{w}| \\{\\mathbf{y}, \\mathbf{x}\\}_{n = 1}^N) = \\frac{p(\\mathbf{y}_{1:N}| \\mathbf{w}, \\mathbf{x}_{1:N})p(\\mathbf{w})}{p(\\mathbf{y}_{1:N})}.$$\n",
    "\n",
    "In the case of a gaussian likelihood and prior, the posterior is also gaussian. This allowed us to calculate the (also gaussian) predictive distribution:\n",
    "\n",
    "$$ p(\\mathbf{y}^*| \\mathbf{x}^*, \\{\\mathbf{y}, \\mathbf{x}\\}_{n = 1}^N) = \\int p(\\mathbf{y}^*| \\mathbf{x}^*, \\mathbf{w}) p(\\mathbf{w}| \\{\\mathbf{y}, \\mathbf{x}\\}_{n = 1}^N) d\\mathbf{w}.$$\n",
    "\n",
    "Selecting the model $\\mathbf{y}_n = \\boldsymbol{\\Phi}\\mathbf{w}$ and placing a distribution over weights $p(\\mathbf{w})$, implies a probability distribution over the output $\\mathbf{y}^*$. What if instead of the roundabout way of stating a model and placing a distribution over its weights, we placed a distribution directly in the space of functions we are trying to fit? Can we produce something like the figure using a gaussian distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
