{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Principal component analysis\n",
    "\n",
    "In the previous discussion we mentioned that in certain datasets, it might be useful to (i) discard low variance, uninformative features of the data to simplify the problem and model or to (ii) find the directions of highest variance to determine the most descriptive features of the data. These two goals point at two correpsonding approaches for dimensionality reduction, which turn out to be equivalent. The first amounts to a basis rotation and disposal of some dimensions, picked such that the sum-of-squares error from the original data is minimised $-$ this is the formulation of reconstruction error minimisation. In the second approach, we discard a number of directions along which the data variance is low, retaining only directions of high variance $-$ this is the formulation of variance maximisation. When using a sum-of-squares as the reconstruction error, the minimum error and maximum variance approaches are equivalent and are really the same method, called **principal component analysis** or PCA.\n",
    "\n",
    "Starting with error minimisation, suppose $\\{\\mathbf{u}_d\\}^D_{d = 1}$ is an *orthonormal* basis, with which we can express any datapoint as a linear combination\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x}_n &= \\sum^D_{d = 1} a_{dn} \\mathbf{u}_d\\\\\n",
    "\\end{align}\n",
    "\n",
    "Separating this in the first \\\\(M\\\\) and remaining \\\\((D - M)\\\\) components\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x}_n =  \\sum^M_{d = 1} a_{dn} \\mathbf{u}_d + \\sum^D_{d = M + 1} a_{dn} \\mathbf{u}_d\\\\\n",
    "\\end{align}\n",
    "\n",
    "and discarding the second sum term involving the \\\\((D - M)\\\\) components, we obtain an approximatate representation of \\\\(\\mathbf{x}_n^\\star\\\\)\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x}_n^\\star =  \\sum^M_{d = 1} a_{dn} \\mathbf{u}_d\\\\\n",
    "\\end{align}\n",
    "\n",
    "The mean squared reconstruction error is then defined as:\n",
    "\n",
    "\\begin{align}\n",
    "E_{rms} &= \\frac{1}{N}\\sum^N_{n = 1} \\big|\\mathbf{x}_n - \\mathbf{x}_n^\\star \\big|^2\\\\\n",
    "\\end{align}\n",
    "\n",
    "and after some manipulation we can write $-$ you can try this as an excercise:\n",
    "\n",
    "\\begin{align}\n",
    "E_{rms} &= \\sum^D_{d = M + 1} \\mathbf{u}_d^\\top \\mathbf{S} \\mathbf{u}_d\\\\\n",
    "~\\\\\n",
    "\\text{ where } \\mathbf{S} &= \\frac{1}{N}\\sum^N_{n = 1}(\\mathbf{x}_n - \\bar{\\mathbf{x}})(\\mathbf{x}_n - \\bar{\\mathbf{x}})^\\top\\\\\\\\\n",
    "\\end{align}\n",
    "\n",
    "<details>\n",
    "<summary>Reconstruction error in detail</summary>\n",
    "<div>\n",
    "    \n",
    "\\begin{align}\n",
    "E_{rms} &= \\frac{1}{N}\\sum^N_{n = 1} \\big|\\mathbf{x}_n - \\mathbf{x}_n^\\star \\big|^2\\\\\n",
    "~\\\\\n",
    "&= \\frac{1}{N}\\sum^N_{n = 1} \\bigg[\\sum^D_{d = M + 1} a_{dn} \\mathbf{u}_d\\bigg]^2\\\\\n",
    "~\\\\\n",
    "&=  \\frac{1}{N}\\sum^N_{n = 1}\\bigg[\\sum^D_{d = M + 1} a_{dn} \\mathbf{u}_d\\bigg]^\\top \\bigg[\\sum^D_{d = M + 1} a_{dn} \\mathbf{u}_d\\bigg]\\\\\n",
    "~\\\\\n",
    "&=  \\frac{1}{N}\\sum^N_{n = 1}\\sum^D_{d = M + 1} a_{dn}^2, \\text{ (using the basis orthonormality)}\\\\\n",
    "~\\\\\n",
    "&=  \\frac{1}{N}\\sum^N_{n = 1}\\sum^D_{d = M + 1} (\\mathbf{u}_d^\\top (\\mathbf{x}_n - \\bar{\\mathbf{x}}))((\\mathbf{x}_n - \\bar{\\mathbf{x}})^\\top\\mathbf{u}_d), \\text{ (using } a_{dn} = \\mathbf{u}_d^\\top (\\mathbf{x}_n - \\bar{\\mathbf{x}}_n))\\\\\n",
    "~\\\\\n",
    "&=  \\sum^D_{d = M + 1} \\mathbf{u}_d^\\top \\mathbf{S} \\mathbf{u}_d, \\text{ (using the definition of } \\mathbf{S})\\\\\n",
    "~\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "Now we seek to minimise $E_{rms}$ with respect to $\\mathbf{u}_d$. However doing so directly would give the vacuous solution $\\mathbf{u}_d = 0$, becuase we haven't constrained the magnitude of the basis vectors which are free to collapse. Requiring $||\\mathbf{u}_d|| = 1$ and using a Lagrange multiplier, we minimise\n",
    "\n",
    "\\begin{align}\n",
    "E = E_{rms} - \\lambda_d(\\mathbf{u}_d^\\top \\mathbf{u}_d - 1) &= \\bigg[\\sum^D_{d = M + 1} \\mathbf{u}_d^\\top \\mathbf{S} \\mathbf{u}_d \\bigg] - \\lambda_d(\\mathbf{u}_d^\\top \\mathbf{u}_d - 1)\\\\\n",
    "\\end{align}\n",
    "\n",
    "with respect to $\\mathbf{u}_d$ to obtain the result $-$ again you can try this as an excercise:\n",
    "\n",
    "\\begin{align}\n",
    "\\boxed{\\mathbf{S} \\mathbf{u}_d = \\lambda_d\\mathbf{u}_d}\\\\\n",
    "\\end{align}\n",
    "\n",
    "<details>\n",
    "<summary>Extremisation in detail</summary>\n",
    "<div>\n",
    "Here is a detailed derivation for the result \\\\(\\mathbf{S} \\mathbf{u}_d = \\lambda_d\\mathbf{u}_d\\\\), with explicit summations.\n",
    "    \n",
    "\\begin{align}\n",
    "\\bigg(\\frac{\\partial E}{\\partial \\mathbf{u}_d}\\bigg)_i &= \\frac{\\partial }{\\partial \\mathbf{u}_{d, i}} \\Bigg[ \\bigg[\\sum^D_{n = M + 1} \\mathbf{u}_n^\\top \\mathbf{S} \\mathbf{u}_n \\bigg] - \\lambda_d(\\mathbf{u}_d^\\top \\mathbf{u}_d - 1)\\Bigg]\\\\\n",
    "~\\\\\\\n",
    "    &= \\frac{\\partial }{\\partial \\mathbf{u}_{d, i}} \\Bigg[\\sum^D_{n = M + 1} \\sum^D_{j = 1}\\sum^D_{k = 1} \\mathbf{u}_{n, j} \\mathbf{S}_{j, k} \\mathbf{u}_{n, k} - \\lambda_d\\bigg[ \\sum^D_{j = 1}\\mathbf{u}_{d, j} \\mathbf{u}_{d, j} - 1\\bigg] \\Bigg]\\\\\n",
    "~\\\\\\\n",
    "    &= 2 \\Bigg[\\sum^D_{j = 1}\\sum^D_{k = 1} \\frac{\\partial \\mathbf{u}_{d, j}} {\\partial \\mathbf{u}_{d, i}}\\mathbf{S}_{j, k} \\mathbf{u}_{d, k} - \\lambda_d\\sum^D_{j = 1} \\frac{\\partial \\mathbf{u}_{d, j}}{\\partial \\mathbf{u}_{d, i}\\mathbf{u}_{d, j}}\\Bigg]\\\\\n",
    "~\\\\\\\n",
    "    &= 2 \\Bigg[\\sum^D_{k = 1} \\delta_{ij} \\mathbf{S}_{j, k} \\mathbf{u}_{d, k} - \\lambda_d\\sum^D_{j = 1} \\mathbf{u}_{d, j} \\delta_{ij} \\Bigg]\\\\\n",
    "~\\\\\\\n",
    "    &= 2 \\Bigg[\\sum^D_{k = 1} \\mathbf{S}_{i, k}\\mathbf{u}_{d, k} - \\lambda_d \\mathbf{u}_{d, i}\\Bigg]\\\\\n",
    "\\end{align}\n",
    "\n",
    "Setting the derivative to $0$:\n",
    "\n",
    "\\\\[\n",
    "\\bigg(\\frac{\\partial E}{\\partial \\mathbf{u}_d}\\bigg)_i = 0\\\\\n",
    "~\\\\\n",
    "\\sum^D_{k = 1} \\mathbf{S}_{i, k}\\mathbf{u}_{d, k} - \\lambda_d \\mathbf{u}_{d, i} = 0\\\\\n",
    "~\\\\\n",
    "~\\\\\n",
    "\\mathbf{S}\\mathbf{u}_{d} - \\lambda_d\\mathbf{u}_{d} = 0\\\\\n",
    "\\\\]\n",
    "\n",
    "Arriving at the result:\n",
    "\\\\[\n",
    "\\boxed{\\mathbf{S}\\mathbf{u}_{d} = \\lambda_d\\mathbf{u}_{d}}\\\\\n",
    "\\\\]\n",
    "\n",
    "</div>\n",
    "</details>\n",
    "\n",
    "Determining $\\mathbf{u}_d$ has therefore turned into an eigenproblem. The $\\mathbf{u}_d$'s which minimize the reconstruction loss are eigenvectors of $\\mathbf{S}$. In addition, each of the corresponding eigenvalues $\\lambda_d$ is equal to the reconstruction loss due to discarding $\\mathbf{u}_d$:\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda_d &= \\mathbf{u}_d^\\top\\mathbf{S}\\mathbf{u}_d =  \\frac{1}{N}\\sum^N_{n = 1} \\mathbf{u}_d^\\top(\\mathbf{x}_n - \\bar{\\mathbf{x}})(\\mathbf{x}_n - \\bar{\\mathbf{x}})^\\top \\mathbf{u}_d\\\\\n",
    "~\\\\\n",
    "\\implies \\sum_{d = M +1}^D \\lambda_d &= E =  \\frac{1}{N}\\sum_{d = M +1}^D\\sum^N_{n = 1} \\mathbf{u}_d^\\top(\\mathbf{x}_n - \\bar{\\mathbf{x}})(\\mathbf{x}_n - \\bar{\\mathbf{x}})^\\top \\mathbf{u}_d\\\\\n",
    "\\end{align}\n",
    "\n",
    "which is a pleasing result. We can implement PCA straightforwardly by solving the eigenproblem $\\mathbf{S} \\mathbf{u}_d = \\lambda_d\\mathbf{u}_d$, and retaining the dimensions $\\mathbf{u}_d$ with the highest eigenvalues $-$ discarding low eigenvalues means low reconstruction loss. Before that however, we will show the equivalence between reconstruction loss minimisation and variance maximisation. The latter amounts to selecting $M$ orthogonal directions such that the variance of the dataset in these directions is maximal:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}_{1:M}(\\{\\mathbf{x}\\}) &=  \\frac{1}{N}\\sum^N_{n = 1}\\bigg[\\sum^M_{d = 1} a_{dn} \\mathbf{u}_d \\bigg]^2\\\\\n",
    "~\\\\\n",
    "&=  \\frac{1}{N} \\sum^N_{n = 1}\\bigg[\\sum^M_{d = 1} a_{dn} \\mathbf{u}_d \\bigg]^\\top \\bigg[\\sum^M_{d = 1} a_{dn} \\mathbf{u}_d \\bigg]\\\\\n",
    "\\end{align}\n",
    "\n",
    "The total variance of the dataset, $\\text{Var}_{1:D}(\\{\\mathbf{x}\\})$, can be expressed as\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Var}_{1:D}(\\{\\mathbf{x}\\}) &=  \\frac{1}{N}\\sum^N_{n = 1}\\bigg[\\sum^D_{d = 1} a_{dn} \\mathbf{u}_d \\bigg]^2\\\\\n",
    "~\\\\\n",
    "&=  \\frac{1}{N}\\sum^N_{n = 1}\\Bigg[\\bigg[\\sum^M_{d = 1} a_{dn} \\mathbf{u}_d \\bigg]^2 + \\bigg[\\sum^D_{d = M + 1} a_{dn} \\mathbf{u}_d \\bigg]^2\\Bigg]\\\\\n",
    "~\\\\\n",
    "&= \\text{Var}_{1:M}(\\{\\mathbf{x}\\}) + \\text{Var}_{M:D}(\\{\\mathbf{x}\\})\\\\\n",
    "\\end{align}\n",
    "\n",
    "where we have used the orthogonality of the basis vectors $\\mathbf{u}_d$. We can read off that the second term $\\text{Var}_{M:D}(\\{\\mathbf{x}\\})$ is equal to the rms reconstruction loss found earlier:\n",
    "\n",
    "\\\\[\n",
    "\\text{Var}_{M:D}(\\{\\mathbf{x}\\}) = E_{rms}\n",
    "\\\\]\n",
    "\n",
    "Considering that $\\text{Var}_{1:D}(\\{\\mathbf{x}\\})$ is constant and independent of the choice of basis, we see that maximizing the variance $\\text{Var}_{1:M}(\\{\\mathbf{x}\\})$ is equivalent to minimising the reconstruction loss $\\text{Var}_{M:D}(\\{\\mathbf{x}\\})$:\n",
    "\n",
    "\\begin{align}\n",
    "\\boxed{\\text{Reconstruction loss minimisation}\\Longleftrightarrow\n",
    "\\text{Variance maximisation}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "    .output {\n",
       "        font-family: \"Georgia\", serif;\n",
       "        align-items: normal;\n",
       "        text-align: normal;\n",
       "    }\n",
       "    \n",
       "    div.output_svg div { margin : auto; }\n",
       "\n",
       "    .div.output_area.MathJax_Display{ text-align: center; }\n",
       "\n",
       "    div.text_cell_render { font-family: \"Georgia\", serif; }\n",
       "    \n",
       "    details {\n",
       "        margin: 20px 0px;\n",
       "        padding: 0px 10px;\n",
       "        border-radius: 3px;\n",
       "        border-style: solid;\n",
       "        border-color: black;\n",
       "        border-width: 2px;\n",
       "    }\n",
       "\n",
       "    details div{padding: 20px 30px;}\n",
       "\n",
       "    details summary{font-size: 18px;}\n",
       "    \n",
       "    table { margin: auto !important; }\n",
       "    \n",
       "    </style>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_functions import *\n",
    "set_notebook_preferences()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
